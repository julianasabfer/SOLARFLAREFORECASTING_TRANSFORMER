{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 - Import libraries o import data\n",
    "'''this plugins is necessary\n",
    "conda install -c conda-forge sunkit-instruments\n",
    "sunpy.instr.goes was moved to sunkit-instruments\n",
    "'''\n",
    "\n",
    "import json\n",
    "import urllib\n",
    "import requests\n",
    "import datetime as dt\n",
    "\n",
    "import os \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import mpld3\n",
    "\n",
    "from sunpy.time import TimeRange\n",
    "import sunkit_instruments.goes_xrs\n",
    "\n",
    "import datetime\n",
    "from sqlalchemy import Time\n",
    "\n",
    "\n",
    "#creates directory to save data, if it does not exist\n",
    "if not os.path.exists('data'):\n",
    "    os.makedirs('data')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2 - Set the date range to search for data and capture GOES events related to explosions\n",
    "\n",
    "def capture_goes(start_date, end_date):\n",
    "\n",
    "    #captures the data saved in the directory, otherwise, generates a new file with the events\n",
    "    if os.path.isfile('data/goes_data.npy'):\n",
    "        goes_events = np.load('data/goes_data.npy', allow_pickle=True, encoding='latin1')      \n",
    "    else:\n",
    "        print(\"Download new data GOES\")\n",
    "        time_range = TimeRange(start_date, end_date)\n",
    "        goes_events = sunkit_instruments.goes_xrs.get_goes_event_list(time_range,\"A1\") #M1, means from M1 classes of solar flares\n",
    "\n",
    "        print('GOES data captured and saved in data/goes_data.npy',len(goes_events),'events.')\n",
    "        np.save(\"data/goes_data.npy\", goes_events)\n",
    "\n",
    "        list_goes = []\n",
    "\n",
    "        for ge in goes_events:\n",
    "            item_goes = []\n",
    "            item_goes.append(ge['event_date'])\n",
    "            item_goes.append(ge['start_time'])\n",
    "            item_goes.append(ge['peak_time'])\n",
    "            item_goes.append(ge['end_time'])\n",
    "            item_goes.append(ge['goes_class'])\n",
    "            item_goes.append(ge['goes_location'])\n",
    "            item_goes.append(ge['noaa_active_region'])\n",
    "            list_goes.append(item_goes)\n",
    "\n",
    "\n",
    "        df = pd.DataFrame(list_goes)\n",
    "        df.to_csv(\"data/goes_list.csv\")\n",
    "\n",
    "\n",
    "        \n",
    "    # NOAA and hARPNUMS active regions map\n",
    "    if os.path.isfile('data/all_harps_with_noaa_ars.txt'):\n",
    "        num_mapper = pd.read_csv('data/all_harps_with_noaa_ars.txt', sep=' ')\n",
    "    else:\n",
    "        num_mapper = pd.read_csv('http://jsoc.stanford.edu/doc/data/hmi/harpnum_to_noaa/all_harps_with_noaa_ars.txt',sep=' ')\n",
    "\n",
    "    \n",
    "\n",
    "    return goes_events, num_mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3 - general functions for data conversion\n",
    "\n",
    "def from_tai_time(tstr):\n",
    "    \"\"\"\n",
    "    Convert the time string to a dt object\n",
    "    \"\"\"\n",
    "    return dt.datetime.strptime(tstr, \"%Y.%m.%d_%H:%M:%S_TAI\")\n",
    "\n",
    "def to_tai_time(date):\n",
    "    \"\"\"\n",
    "    Convert from a dt object to a time string object\n",
    "    \"\"\"\n",
    "    \n",
    "    return date.strftime(\"%Y.%m.%d_%H:%M:%S_TAI\")\n",
    "\n",
    "def round_up(time):\n",
    "    \"\"\"\n",
    "    Rounds up the time to the next time divisible by 12 minutes (JSOC data is saved every 12 minutes)\n",
    "    \n",
    "    Args:\n",
    "    time := td object\n",
    "    \"\"\"\n",
    "    #remainder = time.minute % 12\n",
    "    remainder =  (int( str(time)[14:16] )) % 12\n",
    "\n",
    "    if remainder != 0:\n",
    "        time = time + dt.timedelta(minutes=12-remainder)\n",
    "    return time\n",
    "\n",
    "def convert_noaa_to_harpnum(noaa_ar, num_mapper):\n",
    "    \"\"\"\n",
    "    Converts from a NOAA Active Region to a HARPNUM\n",
    "    Returns harpnum if present, else None if there are no matching harpnums\n",
    "    \n",
    "    Args:\n",
    "    \"\"\"\n",
    "    idx = num_mapper[num_mapper['NOAA_ARS'].str.contains(str(int(noaa_ar)))]\n",
    "    return None if idx.empty else idx.HARPNUM.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4 - Generates URL for search query and captures data\n",
    "# formats for json queries to fetch the feature data and to check if the active region is close to the limb\n",
    "\n",
    "def url_sharp():\n",
    "    url_data = \"http://jsoc.stanford.edu/cgi-bin/ajax/jsoc_info?ds=hmi.sharp_720s[{0}][{1}][? (CODEVER7 !~ '1.1 ')\" + \\\n",
    "        \"and (abs(OBS_VR)< 3500) and (QUALITY<65536) ?]&op=rs_list&key=DATE,DATE_S,DATE_B,DATE__OBS,DATE-OBS,T_OBS,T_REC,USFLUX,MEANGAM,MEANGBT,MEANGBZ,MEANGBH,MEANJZD,\" + \\\n",
    "        \"TOTUSJZ,MEANALP,MEANJZH,TOTUSJH,ABSNJZH,SAVNCPP,MEANPOT,TOTPOT,MEANSHR,SHRGT45,R_VALUE,AREA_ACR\"\n",
    "\n",
    "\n",
    "    ''' \n",
    "    these attributes mentioned in the original work come from the cgem_lorentz base and not hmi.sharp\n",
    "    cgem_lorentz => base\n",
    "    ,TOTBSQ,TOTFZ,EPSZ,TOTFY,TOTFX,EPSY,EPSX\n",
    "    '''\n",
    "\n",
    "    url_limb = \"http://jsoc.stanford.edu/cgi-bin/ajax/jsoc_info?ds=hmi.sharp_cea_720s[{0}][{1}][? (abs(OBS_VR)< 3500)\" + \\\n",
    "        \" and (QUALITY<65536) ?]&op=rs_list&key=CRVAL1,CRLN_OBS\"\n",
    "\n",
    "\n",
    "    url_data = url_data.replace(\" \", \"%20\")\n",
    "    url_limb = url_limb.replace(\" \", \"%20\")\n",
    "\n",
    "    return url_data, url_limb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5 - searches the data in the database by harpnum and checks if there is data related to it\n",
    "def is_clean(harpnum, peak_time, time_deltas, url_data, url_limb):\n",
    "    \"\"\"\n",
    "    Returns True is time/harpnum has the proper data, False otherwise\n",
    "    \n",
    "    Args:\n",
    "    harpnum := the harpnum of this event\n",
    "    peak_time := the peak_time of the event\n",
    "    time_deltas :=a list of timedeltas, where each delta is a required time before the peak time\n",
    "        the first delta is the time of the observation time\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    #create times needed\n",
    "    t_recs = [to_tai_time(peak_time - td) for td in time_deltas]\n",
    "    time = \",\".join(t_recs)\n",
    "    \n",
    "    \n",
    "    response_data = urllib.request.urlopen(url_data.format(harpnum, time))\n",
    "\n",
    "    status_data = response_data.getcode()\n",
    "        \n",
    "    # if there's no response at this time, quit\n",
    "    if status_data!= 200:\n",
    "        print('skip: failed to find JSOC data')\n",
    "        return False, data\n",
    "\n",
    "    # read the JSON output\n",
    "    data = json.loads(response_data.read())\n",
    "    \n",
    "\n",
    "    # if there is no data at this time, quit\n",
    "    if data['count'] == 0:\n",
    "        print('skip: there is no data for HARPNUM',harpnum,'for peak time', peak_time)\n",
    "        return False, data\n",
    "\n",
    "    if 'MISSING' in str(data['keywords']):\n",
    "        print('skip: active region has some missing keywords')\n",
    "        return False, data\n",
    "    \n",
    "    if len(data['keywords'][0]['values']) != len(time_deltas):\n",
    "        print('skip: missing time points for HARPNUM',harpnum,'for peak time', peak_time)\n",
    "        return False, data\n",
    "    \n",
    "    # check to see if the active region is too close to the limb\n",
    "    # we can compute the latitude of an active region in stonyhurst coordinates as follows:\n",
    "    # latitude_stonyhurst = CRVAL1 - CRLN_OBS\n",
    "    # for this we have to query the CEA series \n",
    "    response_limb = urllib.request.urlopen(url_limb.format(harpnum, time))\n",
    "    status_limb = response_limb.getcode()\n",
    "\n",
    "    # if there's no response at this time, quit\n",
    "    if status_limb!= 200:\n",
    "        print('skip: failed to find CEA JSOC data')\n",
    "        return False, data\n",
    "\n",
    "    # read the JSON output\n",
    "    latitude_information = json.loads(response_limb.read())\n",
    "\n",
    "    # if there is no data at this time, quit\n",
    "    if latitude_information['count'] == 0:\n",
    "        print('skip: there is no data (lat) for HARPNUM',harpnum,'for peak time', peak_time)\n",
    "        return False, data\n",
    "\n",
    "    CRVAL1 = float(latitude_information['keywords'][0]['values'][0])\n",
    "    CRLN_OBS = float(latitude_information['keywords'][1]['values'][0])\n",
    "    \n",
    "    if (np.absolute(CRVAL1 - CRLN_OBS) > 70.0):\n",
    "        print('skip: latitude is out of range for',harpnum,'for peak time', peak_time)\n",
    "        return False,data\n",
    "\n",
    "    return True,data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6 - search for positive class\n",
    "\n",
    "def get_positive_class(time_deltas, noaa_events, num_mapper, url_data, url_limb):\n",
    "    \"\"\"\n",
    "    Returns a positive class of harpnums/observation times\n",
    "    \n",
    "    Args:\n",
    "    time_deltas := a list of timedeltas, where each delta is a required time before the peak time\n",
    "        the first delta is the time of the observation time\n",
    "    noaa_events := a list of noaa events that contain positive flares\n",
    "    \"\"\"\n",
    "    pos_class = []\n",
    "    json_pos_class = []\n",
    "    jsontest = []\n",
    "\n",
    "    for event in noaa_events:\n",
    "        harpnum = convert_noaa_to_harpnum(event['noaa_active_region'], num_mapper)\n",
    "        goes_class = event['goes_class']\n",
    "        \n",
    "        if harpnum == None:\n",
    "            print('there are no matching HARPNUMs for', str(int(event['noaa_active_region'])))\n",
    "            continue\n",
    "              \n",
    "            \n",
    "        is_clean_teste, is_clean_data = is_clean(harpnum, event['peak_time'], time_deltas, url_data, url_limb)\n",
    "\n",
    "    \n",
    "        if is_clean_teste:\n",
    "            pos_event = (harpnum, to_tai_time(event['peak_time']), event['goes_class'])\n",
    "            pos_class.append(pos_event)\n",
    " \n",
    "            ###################generate JSON with keywords ################################################################\n",
    "            \n",
    "            line = []\n",
    "            for qt in range(5):\n",
    "\n",
    "                list1 = {'harpnum': harpnum}\n",
    "                line.append(list1)\n",
    "               \n",
    "                list1 = {'class_flare' : goes_class}\n",
    "                line.append(list1)\n",
    "\n",
    "                for qkey in range (len(is_clean_data['keywords'])): #scrolls through the number of times of keywords\n",
    "                    pro = is_clean_data['keywords'][qkey]['name']\n",
    "                    dad = is_clean_data['keywords'][qkey]['values'][qt]\n",
    "\n",
    "                    list1 = {pro: dad}\n",
    "                    line.append(list1)\n",
    "                \n",
    "                jsontest.append(line)\n",
    "                line = []\n",
    "            \n",
    "\n",
    "    ################################### generate CSV with keywords + harpnum ###############################\n",
    "    columnsd = []\n",
    "    line = []\n",
    "    linef = []\n",
    "\n",
    "    for i in jsontest:\n",
    "        columnsd = []\n",
    "        for j in i:\n",
    "            for key, value in j.items():\n",
    "                columnsd.append(key)\n",
    "                line.append(value)\n",
    "        linef.append(line)\n",
    "        line = []\n",
    "        \n",
    "\n",
    "    df = pd.DataFrame(linef, columns = [columnsd])\n",
    "    df.to_csv (r'data/data_sharp_positive.csv', index = False, header=True)\n",
    "\n",
    "    return pos_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#7 get harpunum for time\n",
    "\n",
    "def get_harpnum_from_time(time, max_td, time_deltas, url_data, url_limb):\n",
    "    \n",
    "    url_harpnum = \"http://jsoc.stanford.edu/cgi-bin/ajax/jsoc_info?ds=hmi.sharp_720s[][{}]&op=rs_list&key=HARPNUM\"\n",
    "\n",
    "    datad = []\n",
    "    temp = 0\n",
    "\n",
    "    time_str = to_tai_time(time - max_td) + \",\" + to_tai_time(time) \n",
    "    response = urllib.request.urlopen(url_harpnum.format(time_str))\n",
    "    status = response.getcode()\n",
    "\n",
    "    # if there's no response at this time, quit\n",
    "    if status!= 200:\n",
    "        print('skip: failed to find JSOC data')\n",
    "        return None, temp, datad\n",
    "\n",
    "    # read the JSON output\n",
    "    datad = json.loads(response.read())\n",
    "\n",
    "    # if there is no data at this time, quit\n",
    "    if datad['count'] == 0:\n",
    "        print('skip: there is no data at time', time)\n",
    "        return None, temp, datad\n",
    "    \n",
    "    harpnums = datad['keywords'][0]['values']\n",
    "    \n",
    "    if len(harpnums) == 0:\n",
    "        print('skip: there are no harpnums at time', time)\n",
    "        return None, temp, datad\n",
    "    \n",
    "    for harpnum in set(harpnums):\n",
    "        if harpnums.count(harpnum) == 2:\n",
    "            is_clean_test, is_clean_data = is_clean(harpnum, time, time_deltas)\n",
    "            if is_clean_test:\n",
    "                datad = is_clean_data\n",
    "                return harpnum, time, datad\n",
    "    \n",
    "    return None, temp, datad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#8 - search negative classes\n",
    "\n",
    "def get_negative_class(start_date, end_date, time_deltas, pos_events, max_size, url_data, url_limb):\n",
    "    \"\"\"\n",
    "    Returns a negative class of harpnums/observation times\n",
    "    \n",
    "    Args:\n",
    "    start_date := the date string in the form YYYY-MM-DD\n",
    "    end_date := the date string in the form YYYY-MM-DD\n",
    "    time_deltas := a list of timedeltas, where each delta is a required time before the peak time\n",
    "        the first delta is the time of the observation time\n",
    "    noaa_events := a list of noaa events that contain positive flares\n",
    "    max_size := the maximum size of the negative class\n",
    "    \"\"\"\n",
    "    ## get negative time range\n",
    "    start = dt.datetime.strptime(start_date, '%Y-%m-%d')\n",
    "    end = dt.datetime.strptime(end_date, '%Y-%m-%d')\n",
    "    size = (end-start).days * 24 * 5\n",
    "\n",
    "    \n",
    "    all_times = {start + dt.timedelta(minutes=i*12) for i in range(size)}\n",
    "    \n",
    "   \n",
    "    #remove times within the positive time range tirre .datetime apois peak_time\n",
    "    peak_times = [round_up(event['peak_time']) for event in pos_events]\n",
    "\n",
    "    for time in peak_times:\n",
    "        positive_time = time - dt.timedelta(2)\n",
    "        for _ in range(360):\n",
    "            all_times.discard(positive_time)\n",
    "            positive_time += dt.timedelta(minutes=12)\n",
    "    \n",
    "   \n",
    "    # mix times to avoid any bias and decrease repeats\n",
    "    negative_times = np.random.permutation(list(all_times))\n",
    "    neg_class = []\n",
    "    i = 0\n",
    "    max_td = max(time_deltas)\n",
    "\n",
    "    \n",
    "    ###  imbalanced max_size = len(negative_times)\n",
    "\n",
    "    jsontest = [] \n",
    "\n",
    "    # find viable negative events\n",
    "    while len(neg_class) < max_size and i < len(negative_times):\n",
    "        \n",
    "        info, temp, info_data = get_harpnum_from_time(negative_times[i],max_td, time_deltas, url_data, url_limb)\n",
    "        i += 1\n",
    "        # if there are no harpnums at that time, quit\n",
    "        if info == None:\n",
    "            continue\n",
    "        #harpnum,time = info\n",
    "        harpnum = info\n",
    "        neg_event = (harpnum, to_tai_time(time), None)\n",
    "        neg_class.append(neg_event)\n",
    "        \n",
    "        \n",
    "        ###################generate JSON with keywords #######################################################################\n",
    "\n",
    "        line = []\n",
    "        \n",
    "        for qt in range(5):\n",
    "            list1 = {'harpnum': harpnum}\n",
    "            line.append(list1)\n",
    "            \n",
    "            list1 = {'class_flare' : \"null\"}\n",
    "            line.append(list1)\n",
    "            \n",
    "            for qkey in range (len(info_data['keywords'])): #scrolls through the number of times of keywords\n",
    "                pro = info_data['keywords'][qkey]['name']\n",
    "                dad = info_data['keywords'][qkey]['values'][qt]\n",
    "\n",
    "                list1 = {pro: dad}\n",
    "                line.append(list1)\n",
    "            \n",
    "            jsontest.append(line)\n",
    "            print(\"--------------------------------------------------ITEM JSON--------------------------------------------------\")\n",
    "            print(line)\n",
    "            line = []\n",
    "\n",
    "        \n",
    "        ################################### generate CSV with keywords + o harpnum###############################\n",
    "        columns = []\n",
    "        line = []\n",
    "        linef = []\n",
    "        \n",
    "        \n",
    "        for i_json in jsontest:\n",
    "            columns = []\n",
    "            for j_json in i_json:\n",
    "                for key, value in j_json.items():\n",
    "                    columns.append(key)\n",
    "                    line.append(value)\n",
    "            linef.append(line)\n",
    "            line = []\n",
    "            \n",
    "        \n",
    "\n",
    "        df = pd.DataFrame(linef, columns = [columns])\n",
    "        df.to_csv (r'data/data_sharp_negative.csv', index = False, header=True)\n",
    "        ###################################################################################################################\n",
    "        \n",
    "    return neg_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#9 - captures both positive and negative classes\n",
    "\n",
    "def get_class_events(start_date, end_date, time_deltas, goes_events, num_mapper):\n",
    "    \"\"\"\n",
    "    Returns a positive and negative class of harpnums/observation times\n",
    "    \n",
    "    Args:\n",
    "    start_date := the date string in the form YYYY-MM-DD\n",
    "    end_date := the date string in the form YYYY-MM-DD\n",
    "    time_deltas := a list of timedeltas, where each delta is a required time before the peak time\n",
    "        the first delta is the time of the observation time\n",
    "    \"\"\"\n",
    "    # outputs, each element is a triple (harpnum, observation time, class)\n",
    "    pos_class = []\n",
    "    neg_class = []\n",
    "\n",
    "    url_data, url_limb = url_sharp()\n",
    "    \n",
    "    ## gets the positive class\n",
    "    print(\"Finding positive class ...\")\n",
    "    pos_class.extend(get_positive_class(time_deltas, goes_events, num_mapper, url_data, url_limb))\n",
    "    \n",
    "    \n",
    "    ## get the negative class, for our purposes we will have a 1-to-1 ratio\n",
    "    print(\"Finding negative class...\")\n",
    "    neg_class.extend(get_negative_class(start_date, end_date, time_deltas, goes_events, len(pos_class), url_data, url_limb))\n",
    "    ###neg_class.extend(get_negative_class(start_date, end_date, time_deltas, goes_events, 200))\n",
    "    \n",
    "  \n",
    "    \n",
    "    return pos_class, neg_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding positive class ...\n",
      "skip: there is no data for HARPNUM 1 for peak time 2010-05-01 01:39:00.000\n",
      "skip: there is no data for HARPNUM 2 for peak time 2010-05-01 05:27:00.000\n",
      "skip: missing time points for HARPNUM 1 for peak time 2010-05-01 09:52:00.000\n",
      "skip: missing time points for HARPNUM 1 for peak time 2010-05-02 06:09:00.000\n",
      "skip: latitude is out of range for 1 for peak time 2010-05-02 10:09:00.000\n",
      "skip: latitude is out of range for 1 for peak time 2010-05-02 15:14:00.000\n",
      "skip: latitude is out of range for 1 for peak time 2010-05-02 16:26:00.000\n",
      "skip: missing time points for HARPNUM 1 for peak time 2010-05-02 17:59:00.000\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     goes_event, num_mapper \u001b[38;5;241m=\u001b[39m capture_goes(start_date,end_date)\n\u001b[1;32m---> 18\u001b[0m     pos_class, neg_class \u001b[38;5;241m=\u001b[39m \u001b[43mget_class_events\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_date\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_date\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequired_times\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgoes_event\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_mapper\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m     np\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/pos_class\u001b[39m\u001b[38;5;124m\"\u001b[39m, pos_class)\n\u001b[0;32m     20\u001b[0m     np\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/neg_class\u001b[39m\u001b[38;5;124m\"\u001b[39m, neg_class)\n",
      "Cell \u001b[1;32mIn[26], line 21\u001b[0m, in \u001b[0;36mget_class_events\u001b[1;34m(start_date, end_date, time_deltas, goes_events, num_mapper)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m## gets the positive class\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinding positive class ...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 21\u001b[0m pos_class\u001b[38;5;241m.\u001b[39mextend(\u001b[43mget_positive_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtime_deltas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgoes_events\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_mapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl_limb\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m## get the negative class, for our purposes we will have a 1-to-1 ratio\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinding negative class...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[25], line 45\u001b[0m, in \u001b[0;36mget_positive_class\u001b[1;34m(time_deltas, noaa_events, num_mapper, url_data, url_limb)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m qkey \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m (\u001b[38;5;28mlen\u001b[39m(is_clean_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeywords\u001b[39m\u001b[38;5;124m'\u001b[39m])): \u001b[38;5;66;03m#scrolls through the number of times of keywords\u001b[39;00m\n\u001b[0;32m     44\u001b[0m     pro \u001b[38;5;241m=\u001b[39m is_clean_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeywords\u001b[39m\u001b[38;5;124m'\u001b[39m][qkey][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 45\u001b[0m     dad \u001b[38;5;241m=\u001b[39m \u001b[43mis_clean_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mkeywords\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mqkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvalues\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mqt\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     47\u001b[0m     list1 \u001b[38;5;241m=\u001b[39m {pro: dad}\n\u001b[0;32m     48\u001b[0m     line\u001b[38;5;241m.\u001b[39mappend(list1)\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "#10 - collects data, calling the other functions described above\n",
    "\n",
    "# the times we need to have prior to the peak time\n",
    "#required_times = [dt.timedelta(hours=i) for i in [6,12,18,24,30,36,42,48]] \n",
    "#required_times = [dt.timedelta(hours=i) for i in [0.2, 0.4, 0.6, 0.8, 1]] \n",
    "required_times = [dt.timedelta(hours=i) for i in [6,12,18,24]] \n",
    "start_date = '2010-05-01' #2010-05-01\n",
    "end_date = '2010-06-01'   #2023-03-31\n",
    "\n",
    "# collect data if it does not exist yet\n",
    "if os.path.isfile('data/pos_class.npy') and os.path.isfile('data/neg_class.npy'):\n",
    "    #add, allow_pickle=True, encoding='latin1'\n",
    "    pos_class = np.load('data/pos_class.npy', allow_pickle=True, encoding='latin1')\n",
    "    neg_class = np.load('data/neg_class.npy', allow_pickle=True, encoding='latin1')\n",
    "else:\n",
    "\n",
    "    goes_event, num_mapper = capture_goes(start_date,end_date)\n",
    "    pos_class, neg_class = get_class_events(start_date, end_date, required_times, goes_event, num_mapper)\n",
    "    np.save(\"data/pos_class\", pos_class)\n",
    "    np.save(\"data/neg_class\", neg_class)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#REMOVE DUPLICATES FROM A CSV\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "file = tf.keras.utils\n",
    "raw_df = pd.read_csv(\"data/RAW_DATA/2010-2023-02-28/Classes_A_B_C_M_X/consolidated-duplicate.csv\", sep=\";\")\n",
    "#raw_df.head()\n",
    "\n",
    "raw_df = raw_df.drop_duplicates()\n",
    "\n",
    "raw_df.head()\n",
    "\n",
    "raw_df.to_csv(\"data/DADOS_BRUTOS/2010-2023-02-28/Classes_A_B_C_M_X/consolidated-duplicated-resolve.csv\")\n",
    "\n",
    "\n",
    "#import csv\n",
    "#with open('dados/DADOS_BRUTOS/2010-2023-02-28/Classes_A_B_C_M_X/consolidado-duplicado.csv', 'w') as csvfile:\n",
    "#    csv.writer(csvfile, delimiter=';').write(raw_df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Juliana\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3460: DtypeWarning: Columns (17,22) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 36\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[39m#checa se linha já foi usada\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[39mwhile\u001b[39;00m(avanca_linha):\n\u001b[1;32m---> 36\u001b[0m     \u001b[39mif\u001b[39;00m l_negativa \u001b[39min\u001b[39;00m lista_linhas_usadas \u001b[39mor\u001b[39;00m linha_negativa\u001b[39m.\u001b[39;49mharpnum \u001b[39m!=\u001b[39m linha_positiva\u001b[39m.\u001b[39mharpnum \u001b[39mor\u001b[39;00m linha_negativa\u001b[39m.\u001b[39mClass \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m1\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m     37\u001b[0m         \u001b[39mif\u001b[39;00m l_negativa \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m \u001b[39m<\u001b[39m linha_raw_df:\n\u001b[0;32m     38\u001b[0m             linha_negativa \u001b[39m=\u001b[39m raw_df\u001b[39m.\u001b[39miloc[l_negativa\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m, :]\n",
      "File \u001b[1;32mc:\\Users\\Juliana\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:5473\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5469\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mflags\u001b[39m.\u001b[39mallows_duplicate_labels \u001b[39m=\u001b[39m allows_duplicate_labels\n\u001b[0;32m   5471\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n\u001b[1;32m-> 5473\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getattr__\u001b[39m(\u001b[39mself\u001b[39m, name: \u001b[39mstr\u001b[39m):\n\u001b[0;32m   5474\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   5475\u001b[0m \u001b[39m    After regular attribute access, try looking up the name\u001b[39;00m\n\u001b[0;32m   5476\u001b[0m \u001b[39m    This allows simpler access to columns for interactive use.\u001b[39;00m\n\u001b[0;32m   5477\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m   5478\u001b[0m     \u001b[39m# Note: obj.x will always call obj.__getattribute__('x') prior to\u001b[39;00m\n\u001b[0;32m   5479\u001b[0m     \u001b[39m# calling obj.__getattr__('x').\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#balanceia dados selecionando a amostra negativa mais próxima da positiva\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "file = tf.keras.utils\n",
    "raw_df = pd.read_csv(\"dados/DADOS_TREINAMENTO/2010-2023-03-abcmx.csv\", sep=\";\")\n",
    "linha_raw_df = raw_df[raw_df.columns[0]].count()\n",
    "item0 = raw_df.iloc[0, :]\n",
    "\n",
    "lista = []\n",
    "lista_linhas_usadas = []\n",
    "\n",
    "for i in range(linha_raw_df):\n",
    "    \n",
    "    linha_positiva = raw_df.iloc[i, :]\n",
    "    print(linha_positiva)\n",
    "    if linha_positiva.Class == 1:\n",
    "        #adiciona linha positiva\n",
    "        lista.append(linha_positiva)\n",
    "\n",
    "        #avança proxima linha\n",
    "        pode_avancar = i+i < linha_raw_df\n",
    "        avanca_linha = True\n",
    "\n",
    "        if(pode_avancar):\n",
    "            linha_negativa = raw_df.iloc[i+1, :]  \n",
    "            l_negativa = i+1\n",
    "        else:\n",
    "            if(i-1 >= 0):\n",
    "                linha_negativa = raw_df.iloc[i-1,:]\n",
    "                l_negativa = i - 1\n",
    "\n",
    "        #checa se linha já foi usada\n",
    "        while(avanca_linha):\n",
    "            if l_negativa in lista_linhas_usadas or linha_negativa.harpnum != linha_positiva.harpnum or linha_negativa.Class == \"1\":\n",
    "                if l_negativa + 1 < linha_raw_df:\n",
    "                    linha_negativa = raw_df.iloc[l_negativa+1, :]\n",
    "                    l_negativa = l_negativa + 1\n",
    "                else:\n",
    "                    #percorrer a lista de baixo para cima até o zero\n",
    "                    linha_negativa = raw_df.iloc[l_negativa-1, :]\n",
    "                    l_negativa = l_negativa - 1\n",
    "            else:\n",
    "                avanca_linha = False\n",
    "                lista_linhas_usadas.append(l_negativa)\n",
    "                lista.append(linha_negativa) #adiciona linha negativa\n",
    "    \n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "#cria dataframe novo com a lista e ger o csv\n",
    "df_novo = pd.DataFrame(lista)\n",
    "df_novo.to_csv(\"dados/DADOS_TREINAMENTO/2010-2023-03-abcmx_balanceado.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "11c9965b276cf3313f3c5ed70dbea89491e3ca494ff0df391c074db8741c04fb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
