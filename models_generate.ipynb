{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Juliana\\AppData\\Roaming\\Python\\Python39\\site-packages\\requests\\__init__.py:109: RequestsDependencyWarning: urllib3 (2.0.2) or chardet (5.1.0)/charset_normalizer (3.1.0) doesn't match a supported version!\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#1 - Import libraries\n",
    "\n",
    "import os\n",
    "import tempfile\n",
    "import csv\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from decimal import *\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import sklearn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras import layers # type: ignore\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "\n",
    "import keras.backend as K # type: ignore\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, LSTM, Dense,Dropout, Bidirectional\n",
    "from keras.layers import Conv1D, Conv2D, MaxPooling1D, MaxPooling2D, Flatten, Reshape\n",
    "from tensorflow.keras.layers.experimental import RandomFourierFeatures\n",
    "\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from scipy import interp\n",
    "from itertools import cycle\n",
    "\n",
    "\n",
    "import dask.dataframe as dd\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#2 - additional functions\n",
    "\n",
    "def TSS_matrix_confusion(TN, FP, FN, TP):\n",
    "    TSS = 0\n",
    "    \n",
    "    sensitivity = TP / (TP + FP)\n",
    "    specificity = TN / (FN + TN)\n",
    "    TSS = sensitivity + specificity - 1\n",
    "\n",
    "    return TSS\n",
    "\n",
    "\n",
    "def HSS_matrix_confusion(TN, FP, FN, TP):\n",
    "    HSS = 0\n",
    "    \n",
    "    HSS = ( 2 * ((TP * TN) - (FN * FP)) ) / ( (TP + FN)* (FN + TN) + (TP + FN) * (FP + TN) )\n",
    "    \n",
    "    return HSS\n",
    "\n",
    "def FAR_matrix_confusion(TN, FP, FN, TP):\n",
    "    FAR = 0\n",
    "\n",
    "    FAR = FP/(TP+FP)\n",
    "\n",
    "    return FAR\n",
    "\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to calculate accuracy\n",
    "    -> param y_true: list of true values\n",
    "    -> param y_pred: list of predicted values\n",
    "    -> return: accuracy score\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Intitializing variable to store count of correctly predicted classes\n",
    "    correct_predictions = 0\n",
    "    \n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        \n",
    "        if yt == yp:\n",
    "            \n",
    "            correct_predictions += 1\n",
    "    \n",
    "    #returns accuracy\n",
    "    return correct_predictions / len(y_true)\n",
    "\n",
    "\n",
    "def true_positive(y_true, y_pred):\n",
    "    \n",
    "    tp = 0\n",
    "    \n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        \n",
    "        if yt == 1 and yp == 1:\n",
    "            tp += 1\n",
    "    \n",
    "    return tp\n",
    "\n",
    "\n",
    "def true_negative(y_true, y_pred):\n",
    "    \n",
    "    tn = 0\n",
    "    \n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        \n",
    "        if yt == 0 and yp == 0:\n",
    "            tn += 1\n",
    "            \n",
    "    return tn\n",
    "\n",
    "\n",
    "def false_positive(y_true, y_pred):\n",
    "    \n",
    "    fp = 0\n",
    "    \n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        \n",
    "        if yt == 1 and yp == 0: #01\n",
    "            fp += 1\n",
    "            \n",
    "    return fp\n",
    "\n",
    "\n",
    "def false_negative(y_true, y_pred):\n",
    "    \n",
    "    fn = 0\n",
    "    \n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        \n",
    "        if yt == 0 and yp == 1: #10\n",
    "            fn += 1\n",
    "            \n",
    "    return fn\n",
    "\n",
    "def macro_precision(y_true, y_pred):\n",
    "    \n",
    "    # find the number of classes\n",
    "    num_classes = len(np.unique(y_true))\n",
    "\n",
    "    # initialize precision to 0\n",
    "    precision = 0\n",
    "    \n",
    "    # loop over all classes\n",
    "    for class_ in list(np.unique(y_true)):\n",
    "        \n",
    "        # all classes except current are considered negative\n",
    "        temp_true = [1 if p == class_ else 0 for p in y_true]\n",
    "        temp_pred = [1 if p == class_ else 0 for p in y_pred]\n",
    "        \n",
    "        \n",
    "        # compute true positive for current class\n",
    "        tp = true_positive(temp_true, temp_pred)\n",
    "        \n",
    "        # compute false positive for current class\n",
    "        fp = false_positive(temp_true, temp_pred)\n",
    "        \n",
    "        \n",
    "        # compute precision for current class\n",
    "        temp_precision = tp / (tp + fp + 1e-6)\n",
    "        # keep adding precision for all classes\n",
    "        precision += temp_precision\n",
    "        \n",
    "    # calculate and return average precision over all classes\n",
    "    precision /= num_classes\n",
    "    \n",
    "    return precision\n",
    "\n",
    "\n",
    "def micro_precision(y_true, y_pred):\n",
    "    \n",
    "\n",
    "    # find the number of classes \n",
    "    num_classes = len(np.unique(y_true))\n",
    "    \n",
    "    # initialize tp and fp to 0\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    \n",
    "    # loop over all classes\n",
    "    for class_ in np.unique(y_true):\n",
    "        \n",
    "        # all classes except current are considered negative\n",
    "        temp_true = [1 if p == class_ else 0 for p in y_true]\n",
    "        temp_pred = [1 if p == class_ else 0 for p in y_pred]\n",
    "        \n",
    "        # calculate true positive for current class\n",
    "        # and update overall tp\n",
    "        tp += true_positive(temp_true, temp_pred)\n",
    "        \n",
    "        # calculate false positive for current class\n",
    "        # and update overall tp\n",
    "        fp += false_positive(temp_true, temp_pred)\n",
    "        \n",
    "    # calculate and return overall precision\n",
    "    precision = tp / (tp + fp)\n",
    "    return precision\n",
    "\n",
    "def macro_recall(y_true, y_pred):\n",
    "    \n",
    "    # find the number of classes\n",
    "    num_classes = len(np.unique(y_true))\n",
    "\n",
    "    # initialize recall to 0\n",
    "    recall = 0\n",
    "    \n",
    "    # loop over all classes\n",
    "    for class_ in list(np.unique(y_true)):\n",
    "        \n",
    "        # all classes except current are considered negative\n",
    "        temp_true = [1 if p == class_ else 0 for p in y_true]\n",
    "        temp_pred = [1 if p == class_ else 0 for p in y_pred]\n",
    "        \n",
    "        \n",
    "        # compute true positive for current class\n",
    "        tp = true_positive(temp_true, temp_pred)\n",
    "        \n",
    "        # compute false negative for current class\n",
    "        fn = false_negative(temp_true, temp_pred)\n",
    "        \n",
    "        \n",
    "        # compute recall for current class\n",
    "        temp_recall = tp / (tp + fn + 1e-6)\n",
    "        \n",
    "        # keep adding recall for all classes\n",
    "        recall += temp_recall\n",
    "        \n",
    "    # calculate and return average recall over all classes\n",
    "    recall /= num_classes\n",
    "    \n",
    "    return recall\n",
    "\n",
    "def micro_recall(y_true, y_pred):\n",
    "    \n",
    "\n",
    "    # find the number of classes \n",
    "    num_classes = len(np.unique(y_true))\n",
    "    \n",
    "    # initialize tp and fp to 0\n",
    "    tp = 0\n",
    "    fn = 0\n",
    "    \n",
    "    # loop over all classes\n",
    "    for class_ in np.unique(y_true):\n",
    "        \n",
    "        # all classes except current are considered negative\n",
    "        temp_true = [1 if p == class_ else 0 for p in y_true]\n",
    "        temp_pred = [1 if p == class_ else 0 for p in y_pred]\n",
    "        \n",
    "        # calculate true positive for current class\n",
    "        # and update overall tp\n",
    "        tp += true_positive(temp_true, temp_pred)\n",
    "        \n",
    "        # calculate false negative for current class\n",
    "        # and update overall tp\n",
    "        fn += false_negative(temp_true, temp_pred)\n",
    "        \n",
    "    # calculate and return overall recall\n",
    "    recall = tp / (tp + fn)\n",
    "    return recall\n",
    "\n",
    "def macro_f1(y_true, y_pred):\n",
    "    \n",
    "    # find the number of classes\n",
    "    num_classes = len(np.unique(y_true))\n",
    "\n",
    "    # initialize f1 to 0\n",
    "    f1 = 0\n",
    "    \n",
    "    # loop over all classes\n",
    "    for class_ in list(np.unique(y_true)):\n",
    "        \n",
    "        # all classes except current are considered negative\n",
    "        temp_true = [1 if p == class_ else 0 for p in y_true]\n",
    "        temp_pred = [1 if p == class_ else 0 for p in y_pred]\n",
    "        \n",
    "        \n",
    "        # compute true positive for current class\n",
    "        tp = true_positive(temp_true, temp_pred)\n",
    "        \n",
    "        # compute false negative for current class\n",
    "        fn = false_negative(temp_true, temp_pred)\n",
    "        \n",
    "        # compute false positive for current class\n",
    "        fp = false_positive(temp_true, temp_pred)\n",
    "\n",
    "        # compute recall for current class\n",
    "        temp_recall = tp / (tp + fn + 1e-6)\n",
    "        \n",
    "        # compute precision for current class\n",
    "        temp_precision = tp / (tp + fp + 1e-6)\n",
    "        \n",
    "        \n",
    "        temp_f1 = 2 * temp_precision * temp_recall / (temp_precision + temp_recall + 1e-6)\n",
    "        \n",
    "        # keep adding f1 score for all classes\n",
    "        f1 += temp_f1\n",
    "        \n",
    "    # calculate and return average f1 score over all classes\n",
    "    f1 /= num_classes\n",
    "    \n",
    "    return f1\n",
    "\n",
    "def micro_f1(y_true, y_pred):\n",
    "    \n",
    "\n",
    "    #micro-averaged precision score\n",
    "    P = micro_precision(y_true, y_pred)\n",
    "\n",
    "    #micro-averaged recall score\n",
    "    R = micro_recall(y_true, y_pred)\n",
    "\n",
    "    #micro averaged f1 score\n",
    "    f1 = 2*P*R / (P + R)    \n",
    "\n",
    "    return f1\n",
    "\n",
    "\n",
    "def roc_auc_score_multiclass(actual_class, pred_class, average = \"macro\"):\n",
    "    \n",
    "    #creating a set of all the unique classes using the actual class list\n",
    "    unique_class = set(actual_class)\n",
    "    roc_auc_dict = {}\n",
    "    for per_class in unique_class:\n",
    "        \n",
    "        #creating a list of all the classes except the current class \n",
    "        other_class = [x for x in unique_class if x != per_class]\n",
    "\n",
    "        #marking the current class as 1 and all other classes as 0\n",
    "        new_actual_class = [0 if x in other_class else 1 for x in actual_class]\n",
    "        new_pred_class = [0 if x in other_class else 1 for x in pred_class]\n",
    "\n",
    "        #using the sklearn metrics method to calculate the roc_auc_score\n",
    "        roc_auc = roc_auc_score(new_actual_class, new_pred_class, average = average)\n",
    "        roc_auc_dict[per_class] = roc_auc\n",
    "\n",
    "    return roc_auc_dict\n",
    "\n",
    "\n",
    "\n",
    "#3 - models_algoritms\n",
    "\n",
    "#MLP model\n",
    "def make_model(METRICS, train_features,output_bias=None):\n",
    "    if output_bias is not None:\n",
    "        output_bias = tf.keras.initializers.Constant(output_bias)\n",
    "    \n",
    "\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Dense(\n",
    "            16, activation='relu',\n",
    "            input_shape=(train_features.shape[-1],)),\n",
    "        keras.layers.Dropout(0.5),\n",
    "        keras.layers.Dense(1, activation='sigmoid',\n",
    "                            bias_initializer=output_bias),\n",
    "    ])\n",
    "\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss=keras.losses.BinaryCrossentropy(),\n",
    "        metrics=METRICS)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "#LSTM model\n",
    "def make_model_LSTM(METRICS, output_bias=None):\n",
    "    if output_bias is not None:\n",
    "        output_bias = tf.keras.initializers.Constant(output_bias)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(8,input_shape=(18,1),return_sequences=False))#True = many to many #numero colunas csv\n",
    "    model.add(Dense(2,kernel_initializer='normal',activation='linear'))\n",
    "    model.add(Dense(1,kernel_initializer='normal',activation='linear'))\n",
    "    model.compile(loss='mse',optimizer ='adam',metrics=METRICS)\n",
    "\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def make_model_SVM(METRICS,output_bias=None):\n",
    "    model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(18,)), #number of fields in my csv\n",
    "        RandomFourierFeatures(\n",
    "            output_dim=4096, scale=10.0, kernel_initializer=\"gaussian\"\n",
    "        ),\n",
    "        layers.Dense(units=1),\n",
    "    ]\n",
    "    )  \n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
    "        loss=keras.losses.hinge,\n",
    "        metrics=METRICS,\n",
    "    )\n",
    "        \n",
    "    return model\n",
    "\n",
    "\n",
    "#Transformers Model\n",
    "def transformer_model(input_shape, n_classes):\n",
    "    model = build_model_transformers(\n",
    "        n_classes,\n",
    "        input_shape,\n",
    "        head_size=256, #256\n",
    "        num_heads=2, #4\n",
    "        ff_dim=4, #4\n",
    "        num_transformer_blocks=4, #4\n",
    "        mlp_units=[128], #128\n",
    "        mlp_dropout=0.4, #0.4 \n",
    "        dropout=0.25,  #0.25\n",
    "    )   \n",
    "\n",
    "\n",
    "    loss_function = \"sparse_categorical_crossentropy\"\n",
    "\n",
    "    model.compile(\n",
    "        loss=loss_function,\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
    "        metrics = [\"accuracy\"],\n",
    "        )\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "#transformer-encoder\n",
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    # Normalization and Attention\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
    "    x = layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
    "    )(x, x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    res = x + inputs\n",
    "\n",
    "    # Feed Forward Part\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(res)\n",
    "    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
    "\n",
    "    return x + res\n",
    "\n",
    "#https://keras.io/examples/timeseries/timeseries_classification_transformer/\n",
    "def build_model_transformers(\n",
    "        n_classes,\n",
    "        input_shape,\n",
    "        head_size,\n",
    "        num_heads,\n",
    "        ff_dim,\n",
    "        num_transformer_blocks,\n",
    "        mlp_units,\n",
    "        dropout=0,\n",
    "        mlp_dropout=0,\n",
    "    ):\n",
    "        inputs = keras.Input(shape=input_shape)\n",
    "        x = inputs\n",
    "        for _ in range(num_transformer_blocks):\n",
    "            x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
    "\n",
    "        x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
    "        for dim in mlp_units:\n",
    "            x = layers.Dense(dim, activation=\"relu\")(x) \n",
    "            x = layers.Dropout(mlp_dropout)(x)\n",
    "            \n",
    "        \n",
    "        outputs = layers.Dense(n_classes, activation=\"softmax\")(x)\n",
    "        return keras.Model(inputs, outputs)\n",
    "\n",
    "\n",
    "#4 - Divide datasets and execute models\n",
    "\n",
    "def execute_all_models(set_dataset_base, set_window, set_batch, set_epoch, cleaned_df, set_model_name, set_balancing, set_dataset_division, date_chronological):\n",
    "\n",
    "\n",
    "    #4.1 divide datasets\n",
    "\n",
    "    if set_dataset_division == 'chronological': \n",
    "\n",
    "        datasets = date_chronological\n",
    "\n",
    "        #Datasets for solar cycle A1, A2, A3, A4\n",
    "        if len(datasets) > 6:\n",
    "            train_start_date_1 = datasets[0]\n",
    "            train_end_date_1 = datasets[1]\n",
    "\n",
    "            train_start_date_2 = datasets[2]\n",
    "            train_end_date_2 = datasets[3]\n",
    "\n",
    "            val_start_date = datasets[4]\n",
    "            val_end_date = datasets[5]\n",
    "\n",
    "            test_start_date = datasets[6]\n",
    "            test_end_date = datasets[7]\n",
    "\n",
    "            #create train sets\n",
    "            train_df_1 = cleaned_df.loc[(cleaned_df.T_REC >= train_start_date_1) & (cleaned_df.T_REC <= train_end_date_1)]\n",
    "            train_df_2 = cleaned_df.loc[(cleaned_df.T_REC >= train_start_date_2) & (cleaned_df.T_REC <= train_end_date_2)]\n",
    "            train_df = dd.concat([train_df_1, train_df_2])\n",
    "        #All dataset\n",
    "        else:\n",
    "            train_start_date = datasets[0]\n",
    "            train_end_date = datasets[1]\n",
    "\n",
    "            val_start_date = datasets[2]\n",
    "            val_end_date = datasets[3]\n",
    "\n",
    "            test_start_date = datasets[4]\n",
    "            test_end_date = datasets[5]\n",
    "\n",
    "            #create train sets\n",
    "            train_df = cleaned_df.loc[(cleaned_df.T_REC >= train_start_date) & (cleaned_df.T_REC <= train_end_date)]\n",
    "\n",
    "\n",
    "        #create val and test sets\n",
    "        val_df = cleaned_df.loc[(cleaned_df.T_REC >= val_start_date) & (cleaned_df.T_REC <= val_end_date)]\n",
    "        test_df = cleaned_df.loc[(cleaned_df.T_REC >= test_start_date) & (cleaned_df.T_REC <= test_end_date)]\n",
    "\n",
    "        #sort datasets by date\n",
    "        train_df = train_df.sort_values(by='T_REC')\n",
    "        val_df = val_df.sort_values(by='T_REC')\n",
    "        test_df = test_df.sort_values(by='T_REC')\n",
    "\n",
    "    elif set_dataset_division == 'random':\n",
    "\n",
    "        #separate positive and negative rows\n",
    "        df_neg, df_pos = cleaned_df[(mask:=cleaned_df['Class'] == 0)], cleaned_df[~mask]\n",
    "        \n",
    "        #splits sets randomly\n",
    "        train_df_pos, test_df_pos = df_pos.random_split([0.8, 0.2])  \n",
    "        train_df_pos, val_df_pos = train_df_pos.random_split([0.75, 0.25]) \n",
    "\n",
    "        train_df_neg, test_df_neg = df_neg.random_split([0.8, 0.2]) \n",
    "        train_df_neg, val_df_neg = train_df_neg.random_split([0.75, 0.25]) \n",
    "\n",
    "\n",
    "        train_df =  dd.concat([train_df_pos, train_df_neg])\n",
    "        val_df = dd.concat([val_df_pos,val_df_neg])\n",
    "        test_df = dd.concat([test_df_pos, test_df_neg])\n",
    "    \n",
    "\n",
    "        #order by date dataframe rows\n",
    "        train_df = train_df.sort_values(by='T_REC')\n",
    "        val_df = val_df.sort_values(by='T_REC')\n",
    "        test_df = test_df.sort_values(by='T_REC')\n",
    "        \n",
    "\n",
    "\n",
    "    #4.2 count train, val, test sets\n",
    "\n",
    "    neg_t = len(train_df [train_df.Class == 0])\n",
    "    pos_t = len(train_df [train_df.Class == 1])\n",
    "\n",
    "    print(\"\\n Train: neg=>\", neg_t, \" pos=>\", pos_t)\n",
    "\n",
    "\n",
    "    neg_v = len(val_df [val_df .Class == 0])\n",
    "    pos_v = len(val_df [val_df .Class == 1])\n",
    "\n",
    "    print(\"\\nVal: neg=>\", neg_v, \" pos=>\", pos_v)\n",
    "\n",
    "    neg_te = len(test_df [test_df .Class == 0])\n",
    "    pos_te = len(test_df [test_df .Class == 1])\n",
    "\n",
    "    print(\"\\nTest: neg=>\", neg_te, \" pos=>\", pos_te)\n",
    "\n",
    "    #4.3 Calculate class weights\n",
    "\n",
    "    neg = len(train_df [train_df.Class == 0])\n",
    "    pos = len(train_df [train_df.Class == 1])\n",
    "\n",
    "    total = pos  + neg\n",
    "    weight_for_0 = (1 / neg) * (total / 2.0)\n",
    "    weight_for_1 = (1 / pos) * (total / 2.0)\n",
    "\n",
    "    class_weight = {0: weight_for_0, 1: weight_for_1}\n",
    "\n",
    "    print('Weight class 0: {:.2f}'.format(weight_for_0))\n",
    "    print('Weight class 1: {:.2f}'.format(weight_for_1))\n",
    "    print(class_weight)\n",
    "\n",
    "\n",
    "    #4.4 Balancing training set\n",
    "\n",
    "    #SMOTE balancing\n",
    "    if set_balancing == 'smote':\n",
    "        \n",
    "        train_df['T_REC'] = train_df['T_REC'].apply(lambda x: x.value)\n",
    "        X = train_df.loc[:, train_df.columns != 'Class']\n",
    "        y = train_df.Class\n",
    "        sm = SMOTE(sampling_strategy='auto', k_neighbors=1, random_state=100)\n",
    "        X_res, y_res = sm.fit_resample(X, y)\n",
    "        \n",
    "        train_df = pd.concat([pd.DataFrame(X_res), pd.DataFrame(y_res)], axis=1)\n",
    "        \n",
    "        train_df['T_REC'] = train_df['T_REC'].apply(pd.Timestamp)\n",
    "\n",
    "        train_df = train_df.sort_values(by='T_REC')\n",
    "\n",
    "        train_df = dd.from_pandas(train_df, npartitions=10)\n",
    "\n",
    "    #Oversampling Balancing\n",
    "    elif set_balancing == 'oversampling':\n",
    "\n",
    "        #balancing train_df oversamspling\n",
    "        pos_flare = train_df[train_df['Class']==1]\n",
    "        neg_flare = train_df[train_df['Class']==0]\n",
    "\n",
    "        pos_flare = pos_flare.sample(frac = len(neg_flare) / len(pos_flare), replace = True, random_state=101)\n",
    "        train_df = dd.concat([pos_flare, neg_flare], interleave_partitions=True)   \n",
    "        train_df.sort_values(by='T_REC')\n",
    "          \n",
    "    #Undersampling Balancing\n",
    "    elif set_balancing == 'undersampling':\n",
    "     \n",
    "        pos_flare = train_df[train_df['Class']==1]\n",
    "        neg_flare = train_df[train_df['Class']==0]\n",
    "        \n",
    "        print(len(neg_flare), - len(pos_flare))\n",
    "\n",
    "        neg_flare = neg_flare.sample(frac= 1 / ( len(neg_flare)/len(pos_flare)) , random_state=101)\n",
    "        train_df = dd.concat([pos_flare, neg_flare], interleave_partitions=True)   \n",
    "        train_df.sort_values(by='T_REC')\n",
    "\n",
    "\n",
    "    \n",
    "    #4.5 Count Class after balancing\n",
    "    pos = len(train_df[train_df['Class']==1])\n",
    "    neg = len(train_df[train_df['Class']==0])\n",
    "\n",
    "    total = neg + pos\n",
    "        \n",
    "    print('After balancing - Negative:{} ({:.2f}% of total)\\n'.format(neg, 100 * neg / total))\n",
    "    print('After balancing - Positive: {} ({:.2f}% of total)\\n'.format(pos, 100 * pos / total))\n",
    "    \n",
    "    \n",
    "    #4.6 Clean keys before training\n",
    "    train_df.pop('T_REC')\n",
    "    val_df.pop('T_REC')\n",
    "    test_df.pop('T_REC')\n",
    "\n",
    "    train_df.pop('harpnum')\n",
    "    val_df.pop('harpnum')\n",
    "    test_df.pop('harpnum')\n",
    "\n",
    "    \n",
    "    #4.7 Form np arrays of labels and features.\n",
    "    train_labels = np.array(train_df.pop('Class'))\n",
    "    bool_train_labels = train_labels != 0\n",
    "    val_labels = np.array(val_df.pop('Class'))\n",
    "    test_labels = np.array(test_df.pop('Class'))\n",
    "\n",
    "\n",
    "    #4.8 Create features from each dataset\n",
    "    train_features = np.array(train_df)\n",
    "    val_features = np.array(val_df)\n",
    "    test_features = np.array(test_df)\n",
    "\n",
    "    #4.9 Dataset normalization - StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    train_features = scaler.fit_transform(train_features)\n",
    "    val_features = scaler.transform(val_features)\n",
    "    test_features = scaler.transform(test_features)\n",
    "\n",
    "    train_features = np.clip(train_features, -5, 5)\n",
    "    val_features = np.clip(val_features, -5, 5)\n",
    "    test_features = np.clip(test_features, -5, 5)\n",
    "\n",
    "    #4.10 Set variables model\n",
    "    if set_model_name == \"transformers\" :\n",
    "    \n",
    "        x_train  = train_features\n",
    "        x_val = val_features\n",
    "        x_test = test_features\n",
    "\n",
    "        y_train  = train_labels\n",
    "        y_val = val_labels\n",
    "        y_test = test_labels\n",
    "\n",
    "        y_train = y_train.astype(int)\n",
    "        y_val = y_val.astype(int)\n",
    "        y_test = y_test.astype(int)\n",
    "\n",
    "\n",
    "        n_classes = len(np.unique(y_train))\n",
    "\n",
    "        x_train = x_train.reshape((x_train.shape[0], x_train.shape[1], 1))\n",
    "        x_val = x_val.reshape((x_val.shape[0], x_val.shape[1], 1))\n",
    "        x_test = x_test.reshape((x_test.shape[0], x_test.shape[1], 1))\n",
    "\n",
    "\n",
    "        idx = np.random.permutation(len(x_train))\n",
    "        x_train = x_train[idx]\n",
    "        y_train = y_train[idx]\n",
    "\n",
    "\n",
    "        input_shape = x_train.shape[1:]\n",
    "   \n",
    "        #Create and Compile Model\n",
    "        if set_model_name == \"transformers\":\n",
    "            model = transformer_model(input_shape, n_classes)\n",
    "            model.summary()\n",
    "        else:\n",
    "            model = make_model_SVM()\n",
    "\n",
    "    else:\n",
    "        METRICS = [\"accuracy\"]\n",
    "        if set_model_name == \"mlp\":\n",
    "            model = make_model(METRICS,train_features)\n",
    "        elif set_model_name == \"lstm\":\n",
    "            model = make_model_LSTM(METRICS)\n",
    "        elif set_model_name == \"svm\":\n",
    "            model = make_model_SVM(METRICS)\n",
    "\n",
    "\n",
    "    #4.11 Model Fit\n",
    "    callbacks = [keras.callbacks.ModelCheckpoint(\"fraud_model_at_epoch_{epoch}.keras\")]\n",
    "\n",
    "    if set_model_name == \"transformers\": # or set_model_name == \"svm\" :\n",
    "\n",
    "        #Check use class weight\n",
    "        if set_balancing == 'weight':\n",
    "            model.fit(\n",
    "                x_train,\n",
    "                y_train,\n",
    "                validation_data=(x_val, y_val),\n",
    "                epochs=set_epoch,\n",
    "                batch_size=set_batch,\n",
    "                verbose = 1,\n",
    "                callbacks=callbacks,\n",
    "                class_weight = class_weight,   \n",
    "            )\n",
    "        else:\n",
    "            model.fit(\n",
    "            x_train,\n",
    "            y_train,\n",
    "            validation_data=(x_val, y_val),\n",
    "            epochs=set_epoch,\n",
    "            batch_size=set_batch,\n",
    "            verbose = 1,\n",
    "            callbacks=callbacks,\n",
    "            )\n",
    "\n",
    "    else:\n",
    "        if set_balancing == \"weight\":\n",
    "            model.fit(\n",
    "                    train_features,\n",
    "                    train_labels,\n",
    "                    batch_size=set_batch,\n",
    "                    epochs=set_epoch,\n",
    "                    callbacks= callbacks,\n",
    "                    validation_data=(val_features, val_labels),\n",
    "                    class_weight=class_weight)\n",
    "        else:\n",
    "            model.fit(\n",
    "            train_features,\n",
    "            train_labels,\n",
    "            batch_size=set_batch,\n",
    "            epochs=set_epoch,\n",
    "            callbacks= callbacks,\n",
    "            validation_data=(val_features, val_labels),\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #4.12 Model Evaluate \n",
    "    if set_model_name == \"transformers\": \n",
    "        x_orig = x_test\n",
    "        y_orig = y_test\n",
    "    else:\n",
    "        x_orig = test_features\n",
    "        y_orig = test_labels\n",
    "\n",
    "    SCORE = model.evaluate(x_orig, y_orig, verbose=1)\n",
    "    print('Score:', SCORE)\n",
    "\n",
    "    #get loss metric\n",
    "    loss = -1\n",
    "    for name, value in zip(model.metrics_names, SCORE):\n",
    "        if name == \"loss\":\n",
    "            loss = value\n",
    "\n",
    "\n",
    "    #4.13 Model Predict\n",
    "\n",
    "    y_pred = (model.predict(x_orig) > 0.5).astype(\"int32\")\n",
    "\n",
    "\n",
    "    #Create val predict\n",
    "    class_predict_2 = []\n",
    "    cont_0 = 0\n",
    "    cont_1 = 0\n",
    "    count_tudo = 0\n",
    "\n",
    "\n",
    "    if set_model_name == \"transformers\": \n",
    "        for cp in y_pred:\n",
    "            count_tudo += 1\n",
    "            if cp[0] == 1:\n",
    "                class_predict_2.append(0)\n",
    "                cont_0 = cont_0 + 1\n",
    "            elif cp[1] == 1:\n",
    "                class_predict_2.append(1)\n",
    "                cont_1 = cont_1 + 1\n",
    "            else:\n",
    "                class_predict_2.append(0)\n",
    "    else:\n",
    "        for cp in y_pred:\n",
    "            if cp == 1:\n",
    "                class_predict_2.append(1)\n",
    "            else:\n",
    "                class_predict_2.append(0)\n",
    "\n",
    "    y_pred = class_predict_2\n",
    "\n",
    "  \n",
    "\n",
    "    #4.14 Show confusion matrix and results\n",
    "\n",
    "    labels = [\"NEGATIVE\", \"POSITIVE\"]\n",
    "    cm = confusion_matrix(y_orig, y_pred)\n",
    "\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "\n",
    "    disp.plot()\n",
    "\n",
    "    #4.15 Get metrics' results \n",
    "\n",
    "    tp = true_positive(y_orig, y_pred)\n",
    "    fp = false_positive(y_orig, y_pred)\n",
    "    tn = true_negative(y_orig, y_pred)\n",
    "    fn = false_negative(y_orig, y_pred)\n",
    "\n",
    "    val_TSS = TSS_matrix_confusion(tn, fp, fn, tp)\n",
    "    val_HSS = HSS_matrix_confusion(tn, fp, fn, tp)\n",
    "    val_FAR = FAR_matrix_confusion(tn, fp, fn, tp)\n",
    "\n",
    "    print(\"TP: \", tp)\n",
    "    print(\"TN \", tn)\n",
    "    print(\"FP \", fp) \n",
    "    print(\"FN: \", fn) \n",
    "\n",
    "\n",
    "    print(\"TSS: \", val_TSS)\n",
    "    print(\"HSS: \", val_HSS)\n",
    "    print(\"FAR: \", val_FAR)\n",
    "    print(\"LOSS:\",loss)\n",
    "\n",
    "    print(\"ROC AUC: \", roc_auc_score_multiclass(y_orig, y_pred)[1])\n",
    "    print(\"Accuracy: \", accuracy(y_orig, y_pred))\n",
    "    print(\"Macro precision: \", macro_precision(y_orig, y_pred))\n",
    "    print(\"Micro precision: \", micro_precision(y_orig, y_pred))\n",
    "    print(\"Macro Recall: \", macro_recall(y_orig, y_pred))\n",
    "    print(\"Micro Recall: \", micro_recall(y_orig, y_pred))\n",
    "    print(\"Macro F1: \", macro_f1(y_orig, y_pred))\n",
    "    print(\"Micro F1: \", micro_f1(y_orig, y_pred))\n",
    "\n",
    "\n",
    "    #4.16 - Salve metrics' results in .csv file\n",
    "    metric_list = []\n",
    "    metric_list.append(set_dataset_base)\n",
    "    metric_list.append(set_window)\n",
    "    metric_list.append(set_model_name)\n",
    "    metric_list.append(set_balancing)\n",
    "    metric_list.append(set_dataset_division)\n",
    "    metric_list.append(set_batch)\n",
    "    metric_list.append(set_epoch)\n",
    "    metric_list.append(tp)\n",
    "    metric_list.append(tn)\n",
    "    metric_list.append(fp)\n",
    "    metric_list.append(fn)\n",
    "    metric_list.append(accuracy(y_orig, y_pred))\n",
    "    metric_list.append(val_TSS)\n",
    "    metric_list.append(val_HSS)\n",
    "    metric_list.append(roc_auc_score_multiclass(y_orig, y_pred)[1])\n",
    "    metric_list.append(val_FAR)\n",
    "    metric_list.append(loss)\n",
    "    metric_list.append(macro_precision(y_orig, y_pred))\n",
    "    metric_list.append(micro_precision(y_orig, y_pred))\n",
    "    metric_list.append(macro_recall(y_orig, y_pred))\n",
    "    metric_list.append(micro_recall(y_orig, y_pred))\n",
    "    metric_list.append(macro_f1(y_orig, y_pred))\n",
    "    metric_list.append(micro_f1(y_orig, y_pred))\n",
    "\n",
    "    \n",
    "    df_results = pd.DataFrame(metric_list).T\n",
    "    df_results.columns = [\"dataset\", \"window\", \"model\", \"balancing\", \"sep_datasets\", \"batch\", \"epoch\", \"TP\", \"TN\", \"FP\", \"FN\", \"Acurácia\", \"TSS\" ,\"HSS\" ,\"AUC/ROC\" ,\"FAR\" ,\"LOSS\" ,\"Macro Precision\" ,\"Micro Precision\" ,\"Macro Recall\" ,\"Micro Recall\" ,\"Macro F1\" ,\"Micro F1\"]\n",
    "\n",
    "    if os.path.isfile('data/result_models_test.csv'):\n",
    "        df_results.to_csv('data/result_models_test.csv', index= False, header=False, mode = 'a')\n",
    "    else:\n",
    "        df_results.to_csv('data/result_models_test.csv', index= False, header=True, mode = 'a')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lenght before drop nan values:  118566\n",
      "Lenght after drop nan values:  117066\n",
      "A1  -  24h  -  64  -  10  -  transformers  -  weight  - \n",
      "\n",
      " Train: neg=> 8435  pos=> 73\n",
      "\n",
      "Val: neg=> 1209  pos=> 4\n",
      "\n",
      "Test: neg=> 2365  pos=> 26\n",
      "Weight class 0: 0.50\n",
      "Weight class 1: 58.27\n",
      "{0: 0.5043272080616479, 1: 58.273972602739725}\n",
      "After balancing - Negative:8435 (99.14% of total)\n",
      "\n",
      "After balancing - Positive: 73 (0.86% of total)\n",
      "\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 10, 1)]      0           []                               \n",
      "                                                                                                  \n",
      " layer_normalization_16 (LayerN  (None, 10, 1)       2           ['input_3[0][0]']                \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_8 (MultiH  (None, 10, 1)       3585        ['layer_normalization_16[0][0]', \n",
      " eadAttention)                                                    'layer_normalization_16[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_18 (Dropout)           (None, 10, 1)        0           ['multi_head_attention_8[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_16 (TFOpL  (None, 10, 1)       0           ['dropout_18[0][0]',             \n",
      " ambda)                                                           'input_3[0][0]']                \n",
      "                                                                                                  \n",
      " layer_normalization_17 (LayerN  (None, 10, 1)       2           ['tf.__operators__.add_16[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv1d_16 (Conv1D)             (None, 10, 4)        8           ['layer_normalization_17[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_19 (Dropout)           (None, 10, 4)        0           ['conv1d_16[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_17 (Conv1D)             (None, 10, 1)        5           ['dropout_19[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_17 (TFOpL  (None, 10, 1)       0           ['conv1d_17[0][0]',              \n",
      " ambda)                                                           'tf.__operators__.add_16[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_18 (LayerN  (None, 10, 1)       2           ['tf.__operators__.add_17[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_9 (MultiH  (None, 10, 1)       3585        ['layer_normalization_18[0][0]', \n",
      " eadAttention)                                                    'layer_normalization_18[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_20 (Dropout)           (None, 10, 1)        0           ['multi_head_attention_9[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_18 (TFOpL  (None, 10, 1)       0           ['dropout_20[0][0]',             \n",
      " ambda)                                                           'tf.__operators__.add_17[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_19 (LayerN  (None, 10, 1)       2           ['tf.__operators__.add_18[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv1d_18 (Conv1D)             (None, 10, 4)        8           ['layer_normalization_19[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_21 (Dropout)           (None, 10, 4)        0           ['conv1d_18[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_19 (Conv1D)             (None, 10, 1)        5           ['dropout_21[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_19 (TFOpL  (None, 10, 1)       0           ['conv1d_19[0][0]',              \n",
      " ambda)                                                           'tf.__operators__.add_18[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_20 (LayerN  (None, 10, 1)       2           ['tf.__operators__.add_19[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_10 (Multi  (None, 10, 1)       3585        ['layer_normalization_20[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_20[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_22 (Dropout)           (None, 10, 1)        0           ['multi_head_attention_10[0][0]']\n",
      "                                                                                                  \n",
      " tf.__operators__.add_20 (TFOpL  (None, 10, 1)       0           ['dropout_22[0][0]',             \n",
      " ambda)                                                           'tf.__operators__.add_19[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_21 (LayerN  (None, 10, 1)       2           ['tf.__operators__.add_20[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv1d_20 (Conv1D)             (None, 10, 4)        8           ['layer_normalization_21[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_23 (Dropout)           (None, 10, 4)        0           ['conv1d_20[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_21 (Conv1D)             (None, 10, 1)        5           ['dropout_23[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_21 (TFOpL  (None, 10, 1)       0           ['conv1d_21[0][0]',              \n",
      " ambda)                                                           'tf.__operators__.add_20[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_22 (LayerN  (None, 10, 1)       2           ['tf.__operators__.add_21[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_11 (Multi  (None, 10, 1)       3585        ['layer_normalization_22[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_22[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_24 (Dropout)           (None, 10, 1)        0           ['multi_head_attention_11[0][0]']\n",
      "                                                                                                  \n",
      " tf.__operators__.add_22 (TFOpL  (None, 10, 1)       0           ['dropout_24[0][0]',             \n",
      " ambda)                                                           'tf.__operators__.add_21[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_23 (LayerN  (None, 10, 1)       2           ['tf.__operators__.add_22[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv1d_22 (Conv1D)             (None, 10, 4)        8           ['layer_normalization_23[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_25 (Dropout)           (None, 10, 4)        0           ['conv1d_22[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_23 (Conv1D)             (None, 10, 1)        5           ['dropout_25[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_23 (TFOpL  (None, 10, 1)       0           ['conv1d_23[0][0]',              \n",
      " ambda)                                                           'tf.__operators__.add_22[0][0]']\n",
      "                                                                                                  \n",
      " global_average_pooling1d_2 (Gl  (None, 10)          0           ['tf.__operators__.add_23[0][0]']\n",
      " obalAveragePooling1D)                                                                            \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 128)          1408        ['global_average_pooling1d_2[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dropout_26 (Dropout)           (None, 128)          0           ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 2)            258         ['dropout_26[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 16,074\n",
      "Trainable params: 16,074\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/10\n",
      "133/133 [==============================] - 14s 85ms/step - loss: 0.5261 - accuracy: 0.8980 - val_loss: 0.1961 - val_accuracy: 0.9481\n",
      "Epoch 2/10\n",
      "133/133 [==============================] - 11s 84ms/step - loss: 0.3803 - accuracy: 0.9144 - val_loss: 0.2238 - val_accuracy: 0.9044\n",
      "Epoch 3/10\n",
      "133/133 [==============================] - 11s 83ms/step - loss: 0.3308 - accuracy: 0.8822 - val_loss: 0.1921 - val_accuracy: 0.9159\n",
      "Epoch 4/10\n",
      "133/133 [==============================] - 11s 84ms/step - loss: 0.2872 - accuracy: 0.8889 - val_loss: 0.2067 - val_accuracy: 0.8937\n",
      "Epoch 5/10\n",
      "133/133 [==============================] - 10s 76ms/step - loss: 0.2854 - accuracy: 0.8739 - val_loss: 0.1893 - val_accuracy: 0.9027\n",
      "Epoch 6/10\n",
      "133/133 [==============================] - 11s 81ms/step - loss: 0.2981 - accuracy: 0.8754 - val_loss: 0.2210 - val_accuracy: 0.8722\n",
      "Epoch 7/10\n",
      "133/133 [==============================] - 11s 83ms/step - loss: 0.2904 - accuracy: 0.8704 - val_loss: 0.2105 - val_accuracy: 0.8772\n",
      "Epoch 8/10\n",
      "133/133 [==============================] - 11s 83ms/step - loss: 0.2927 - accuracy: 0.8642 - val_loss: 0.2315 - val_accuracy: 0.8623\n",
      "Epoch 9/10\n",
      "133/133 [==============================] - 10s 76ms/step - loss: 0.2721 - accuracy: 0.8718 - val_loss: 0.2047 - val_accuracy: 0.8805\n",
      "Epoch 10/10\n",
      "133/133 [==============================] - 12s 88ms/step - loss: 0.2793 - accuracy: 0.8682 - val_loss: 0.2181 - val_accuracy: 0.8689\n",
      "75/75 [==============================] - 1s 9ms/step - loss: 0.3620 - accuracy: 0.8603\n",
      "Score: [0.36201438307762146, 0.8603094816207886]\n",
      "75/75 [==============================] - 1s 10ms/step\n",
      "TP:  26\n",
      "TN  2031\n",
      "FP  0\n",
      "FN:  334\n",
      "TSS:  0.8587737843551797\n",
      "HSS:  0.06673491052471944\n",
      "FAR:  0.0\n",
      "LOSS: 0.36201438307762146\n",
      "ROC AUC:  0.9293868921775899\n",
      "Accuracy:  0.8603094939355918\n",
      "Macro precision:  0.9293868727652623\n",
      "Micro precision:  0.8603094939355918\n",
      "Macro Recall:  0.5361111107646183\n",
      "Micro Recall:  0.8603094939355918\n",
      "Macro F1:  0.5293681200343399\n",
      "Micro F1:  0.8603094939355918\n",
      "A1  -  24h  -  64  -  10  -  transformers  -  oversampling  - \n",
      "\n",
      " Train: neg=> 8435  pos=> 73\n",
      "\n",
      "Val: neg=> 1209  pos=> 4\n",
      "\n",
      "Test: neg=> 2365  pos=> 26\n",
      "Weight class 0: 0.50\n",
      "Weight class 1: 58.27\n",
      "{0: 0.5043272080616479, 1: 58.273972602739725}\n",
      "After balancing - Negative:8435 (49.99% of total)\n",
      "\n",
      "After balancing - Positive: 8437 (50.01% of total)\n",
      "\n",
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_4 (InputLayer)           [(None, 10, 1)]      0           []                               \n",
      "                                                                                                  \n",
      " layer_normalization_24 (LayerN  (None, 10, 1)       2           ['input_4[0][0]']                \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_12 (Multi  (None, 10, 1)       3585        ['layer_normalization_24[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_24[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_27 (Dropout)           (None, 10, 1)        0           ['multi_head_attention_12[0][0]']\n",
      "                                                                                                  \n",
      " tf.__operators__.add_24 (TFOpL  (None, 10, 1)       0           ['dropout_27[0][0]',             \n",
      " ambda)                                                           'input_4[0][0]']                \n",
      "                                                                                                  \n",
      " layer_normalization_25 (LayerN  (None, 10, 1)       2           ['tf.__operators__.add_24[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv1d_24 (Conv1D)             (None, 10, 4)        8           ['layer_normalization_25[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_28 (Dropout)           (None, 10, 4)        0           ['conv1d_24[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_25 (Conv1D)             (None, 10, 1)        5           ['dropout_28[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_25 (TFOpL  (None, 10, 1)       0           ['conv1d_25[0][0]',              \n",
      " ambda)                                                           'tf.__operators__.add_24[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_26 (LayerN  (None, 10, 1)       2           ['tf.__operators__.add_25[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_13 (Multi  (None, 10, 1)       3585        ['layer_normalization_26[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_26[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_29 (Dropout)           (None, 10, 1)        0           ['multi_head_attention_13[0][0]']\n",
      "                                                                                                  \n",
      " tf.__operators__.add_26 (TFOpL  (None, 10, 1)       0           ['dropout_29[0][0]',             \n",
      " ambda)                                                           'tf.__operators__.add_25[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_27 (LayerN  (None, 10, 1)       2           ['tf.__operators__.add_26[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv1d_26 (Conv1D)             (None, 10, 4)        8           ['layer_normalization_27[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_30 (Dropout)           (None, 10, 4)        0           ['conv1d_26[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_27 (Conv1D)             (None, 10, 1)        5           ['dropout_30[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_27 (TFOpL  (None, 10, 1)       0           ['conv1d_27[0][0]',              \n",
      " ambda)                                                           'tf.__operators__.add_26[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_28 (LayerN  (None, 10, 1)       2           ['tf.__operators__.add_27[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_14 (Multi  (None, 10, 1)       3585        ['layer_normalization_28[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_28[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_31 (Dropout)           (None, 10, 1)        0           ['multi_head_attention_14[0][0]']\n",
      "                                                                                                  \n",
      " tf.__operators__.add_28 (TFOpL  (None, 10, 1)       0           ['dropout_31[0][0]',             \n",
      " ambda)                                                           'tf.__operators__.add_27[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_29 (LayerN  (None, 10, 1)       2           ['tf.__operators__.add_28[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv1d_28 (Conv1D)             (None, 10, 4)        8           ['layer_normalization_29[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_32 (Dropout)           (None, 10, 4)        0           ['conv1d_28[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_29 (Conv1D)             (None, 10, 1)        5           ['dropout_32[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_29 (TFOpL  (None, 10, 1)       0           ['conv1d_29[0][0]',              \n",
      " ambda)                                                           'tf.__operators__.add_28[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_30 (LayerN  (None, 10, 1)       2           ['tf.__operators__.add_29[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_15 (Multi  (None, 10, 1)       3585        ['layer_normalization_30[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_30[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_33 (Dropout)           (None, 10, 1)        0           ['multi_head_attention_15[0][0]']\n",
      "                                                                                                  \n",
      " tf.__operators__.add_30 (TFOpL  (None, 10, 1)       0           ['dropout_33[0][0]',             \n",
      " ambda)                                                           'tf.__operators__.add_29[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_31 (LayerN  (None, 10, 1)       2           ['tf.__operators__.add_30[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv1d_30 (Conv1D)             (None, 10, 4)        8           ['layer_normalization_31[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_34 (Dropout)           (None, 10, 4)        0           ['conv1d_30[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_31 (Conv1D)             (None, 10, 1)        5           ['dropout_34[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_31 (TFOpL  (None, 10, 1)       0           ['conv1d_31[0][0]',              \n",
      " ambda)                                                           'tf.__operators__.add_30[0][0]']\n",
      "                                                                                                  \n",
      " global_average_pooling1d_3 (Gl  (None, 10)          0           ['tf.__operators__.add_31[0][0]']\n",
      " obalAveragePooling1D)                                                                            \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 128)          1408        ['global_average_pooling1d_3[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dropout_35 (Dropout)           (None, 128)          0           ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 2)            258         ['dropout_35[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 16,074\n",
      "Trainable params: 16,074\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/10\n",
      "264/264 [==============================] - 24s 77ms/step - loss: 0.5610 - accuracy: 0.7021 - val_loss: 0.2806 - val_accuracy: 0.9349\n",
      "Epoch 2/10\n",
      "264/264 [==============================] - 18s 69ms/step - loss: 0.3191 - accuracy: 0.8757 - val_loss: 0.2118 - val_accuracy: 0.9233\n",
      "Epoch 3/10\n",
      "264/264 [==============================] - 20s 76ms/step - loss: 0.2826 - accuracy: 0.8810 - val_loss: 0.1919 - val_accuracy: 0.9209\n",
      "Epoch 4/10\n",
      "264/264 [==============================] - 19s 74ms/step - loss: 0.2713 - accuracy: 0.8881 - val_loss: 0.1974 - val_accuracy: 0.9068\n",
      "Epoch 5/10\n",
      "264/264 [==============================] - 18s 69ms/step - loss: 0.2645 - accuracy: 0.8950 - val_loss: 0.1984 - val_accuracy: 0.8904\n",
      "Epoch 6/10\n",
      "264/264 [==============================] - 18s 68ms/step - loss: 0.2601 - accuracy: 0.8972 - val_loss: 0.1849 - val_accuracy: 0.8978\n",
      "Epoch 7/10\n",
      "264/264 [==============================] - 18s 68ms/step - loss: 0.2552 - accuracy: 0.9004 - val_loss: 0.1953 - val_accuracy: 0.8829\n",
      "Epoch 8/10\n",
      "264/264 [==============================] - 19s 71ms/step - loss: 0.2510 - accuracy: 0.9018 - val_loss: 0.1776 - val_accuracy: 0.9011\n",
      "Epoch 9/10\n",
      "264/264 [==============================] - 20s 77ms/step - loss: 0.2467 - accuracy: 0.9044 - val_loss: 0.1946 - val_accuracy: 0.8796\n",
      "Epoch 10/10\n",
      "264/264 [==============================] - 20s 76ms/step - loss: 0.2449 - accuracy: 0.9021 - val_loss: 0.1702 - val_accuracy: 0.9077\n",
      "75/75 [==============================] - 1s 9ms/step - loss: 0.3113 - accuracy: 0.8678\n",
      "Score: [0.3112831115722656, 0.8678377270698547]\n",
      "75/75 [==============================] - 1s 9ms/step\n",
      "TP:  26\n",
      "TN  2049\n",
      "FP  0\n",
      "FN:  316\n",
      "TSS:  0.866384778012685\n",
      "HSS:  0.07058084722454074\n",
      "FAR:  0.0\n",
      "LOSS: 0.3112831115722656\n",
      "ROC AUC:  0.9331923890063425\n",
      "Accuracy:  0.8678377248013384\n",
      "Macro precision:  0.933192369592406\n",
      "Micro precision:  0.8678377248013384\n",
      "Macro Recall:  0.5380116955512659\n",
      "Micro Recall:  0.8678377248013384\n",
      "Macro F1:  0.5348566618396396\n",
      "Micro F1:  0.8678377248013384\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "#set variables -------------------------------------------------------------------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "#path dataset\n",
    "separator = ','\n",
    "url_data = 'data/sharp_abcmx-nf-all_6h_2010-01-01_2024-04-30.csv'\n",
    "\n",
    "#Coluns Delete in CSV file\n",
    "#list_col_delete = ['DATE','DATE_S','DATE_B','DATE__OBS','DATE-OBS','T_OBS','OBS_VR', 'QUALITY', 'Class_Flare']\n",
    "\n",
    "#Attributes' redution \n",
    "list_col_delete = ['DATE','DATE_S','DATE_B','DATE__OBS','DATE-OBS','T_OBS','OBS_VR', 'QUALITY', 'Class_Flare', 'MEANGAM','MEANGBT','MEANGBZ','MEANSHR','MEANGBH','MEANJZH', 'MEANJZD','MEANALP'] \n",
    "\n",
    "date_chronological_empty = []\n",
    "#date_chronological = [\"2010-01-01 00:00:00\", \"2014-10-26 23:59:00\", \"2014-10-27 00:00:00\", \"2015-10-30 23:59:00\", \"2015-10-31 00:00:00\", \"2023-12-31 23:59:00\"]\n",
    "\n",
    "#date A1 - 2010-2011\n",
    "date_chronological_A1 = [\"2010-05-03 00:00:00\", \"2010-10-31 23:59:59\", \"2011-06-01 00:00:00\", \"2011-12-31 23:59:59\", \"2010-11-01 00:00:00\", \"2011-01-31 23:59:59\", \"2011-02-21 00:00:00\", \"2011-05-31 23:59:59\"]\n",
    "\n",
    "#date A2 - 2012 - 2013\n",
    "date_chronological_A2 = [\"2012-01-01 00:00:00\", \"2012-08-31 23:59:59\", \"2013-07-01 00:00:00\", \"2014-08-31 23:59:59\", \"2012-09-01 00:00:00\", \"2013-01-31 23:59:59\", \"2013-02-01 00:00:00\", \"2013-06-30 23:59:59\"]\n",
    "\n",
    "#date A3  - 2014-2016\n",
    "date_chronological_A3 = [\"2014-09-01 00:00:00\", \"2015-03-31 23:59:59\", \"2015-10-01 00:00:00\", \"2016-06-30 23:59:59\", \"2015-04-01 00:00:00\", \"2015-06-30 23:59:59\", \"2015-07-01 00:00:00\", \"2015-09-30 23:59:59\"]\n",
    "\n",
    "#date A4 - 2016-2018\n",
    "date_chronological_A4 = [\"2016-07-01 00:00:00\", \"2017-02-28 23:59:59\", \"2017-09-01 00:00:00\", \"2018-07-31 23:59:59\", \"2017-03-01 00:00:00\", \"2017-05-31 23:59:59\", \"2017-06-01 00:00:00\", \"2017-08-31 23:59:59\"]\n",
    "\n",
    "#set parameters before training\n",
    "set_dataset_base = \"all-abcmx\"\n",
    "set_window = \"24h\"\n",
    "set_epoch  = 10\n",
    "set_batch = 64\n",
    "set_model_name = ['transformers'] # ['mlp', 'svm','lstm','transformers']\n",
    "set_dataset_base = ['A1'] #['all', 'A1', 'A2', 'A3', 'A4']\n",
    "set_balancing = ['weight', 'oversampling'] # ['weight', 'undersampling', 'oversampling', 'smote']\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "#config datetime format    \n",
    "#custom_date_parser = lambda x: datetime.strptime(x, \"%Y.%m.%d_%H:%M:%S_TAI\") \n",
    "custom_date_parser = lambda x: datetime.strptime(x, '%Y-%m-%d %H:%M:%S') \n",
    " \n",
    "#load csv \n",
    "raw_df = pd.read_csv(url_data, sep=',')\n",
    "raw_df['T_REC'] = pd.to_datetime(raw_df['T_REC'], format='%Y-%m-%d %H:%M:%S')\n",
    "raw_df.sort_values(by='T_REC')\n",
    "\n",
    "#drop nan values\n",
    "print(\"Lenght before drop nan values: \", len(raw_df))\n",
    "raw_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "raw_df.dropna(subset=raw_df.columns, inplace=True)\n",
    "raw_df.drop\n",
    "print(\"Lenght after drop nan values: \", len(raw_df))\n",
    "\n",
    "\n",
    "#convert pandas' dataset to dask dataframe\n",
    "cleaned_df = dd.from_pandas(raw_df, npartitions=10)\n",
    "\n",
    "#delete unused columns\n",
    "for lcd in list_col_delete:\n",
    "    cleaned_df.pop(lcd)\n",
    "\n",
    "#count row class\n",
    "neg = len(cleaned_df [cleaned_df.Class == 0])\n",
    "pos = len(cleaned_df [cleaned_df.Class == 1])\n",
    "\n",
    "\n",
    "#training models\n",
    "for smn in set_model_name:\n",
    "\n",
    "    for sdb in set_dataset_base:\n",
    "\n",
    "        for sba in set_balancing:\n",
    "            print(sdb, ' - ', set_window,  ' - ', set_batch,  ' - ', set_epoch, ' - ', smn,  ' - ', sba,  ' - ')\n",
    "            if sdb == 'all':\n",
    "                execute_all_models(sdb, set_window, set_batch, set_epoch, cleaned_df, smn, sba, \"random\", date_chronological_empty)\n",
    "            elif sdb == 'A1':\n",
    "                execute_all_models(sdb, set_window, set_batch, set_epoch, cleaned_df, smn, sba, \"chronological\", date_chronological_A1)\n",
    "            elif sdb == 'A2':\n",
    "                execute_all_models(sdb, set_window, set_batch, set_epoch, cleaned_df, smn, sba, \"chronological\", date_chronological_A2)\n",
    "            elif sdb == 'A3':\n",
    "                execute_all_models(sdb, set_window, set_batch, set_epoch, cleaned_df, smn, sba, \"chronological\", date_chronological_A3)\n",
    "            elif sdb == 'A4':\n",
    "                execute_all_models(sdb, set_window, set_batch, set_epoch, cleaned_df, smn, sba, \"chronological\",  date_chronological_A4)\n",
    "            \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
