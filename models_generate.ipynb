{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 - Import libraries\n",
    "\n",
    "import os\n",
    "import tempfile\n",
    "import csv\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from decimal import *\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import sklearn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras import layers # type: ignore\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "\n",
    "import keras.backend as K # type: ignore\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, LSTM, Dense,Dropout, Bidirectional\n",
    "from keras.layers import Conv1D, Conv2D, MaxPooling1D, MaxPooling2D, Flatten, Reshape\n",
    "from tensorflow.keras.layers.experimental import RandomFourierFeatures\n",
    "\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from scipy import interp\n",
    "from itertools import cycle\n",
    "\n",
    "\n",
    "import dask.dataframe as dd\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#2 - additional functions\n",
    "\n",
    "def TSS_matrix_confusion(TN, FP, FN, TP):\n",
    "    TSS = 0\n",
    "    \n",
    "    sensitivity = TP / (TP + FP)\n",
    "    specificity = TN / (FN + TN)\n",
    "    TSS = sensitivity + specificity - 1\n",
    "\n",
    "    return TSS\n",
    "\n",
    "\n",
    "def HSS_matrix_confusion(TN, FP, FN, TP):\n",
    "    HSS = 0\n",
    "    \n",
    "    HSS = ( 2 * ((TP * TN) - (FN * FP)) ) / ( (TP + FN)* (FN + TN) + (TP + FN) * (FP + TN) )\n",
    "    \n",
    "    return HSS\n",
    "\n",
    "def FAR_matrix_confusion(TN, FP, FN, TP):\n",
    "    FAR = 0\n",
    "\n",
    "    FAR = FP/(TP+FP)\n",
    "\n",
    "    return FAR\n",
    "\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to calculate accuracy\n",
    "    -> param y_true: list of true values\n",
    "    -> param y_pred: list of predicted values\n",
    "    -> return: accuracy score\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Intitializing variable to store count of correctly predicted classes\n",
    "    correct_predictions = 0\n",
    "    \n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        \n",
    "        if yt == yp:\n",
    "            \n",
    "            correct_predictions += 1\n",
    "    \n",
    "    #returns accuracy\n",
    "    return correct_predictions / len(y_true)\n",
    "\n",
    "\n",
    "def true_positive(y_true, y_pred):\n",
    "    \n",
    "    tp = 0\n",
    "    \n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        \n",
    "        if yt == 1 and yp == 1:\n",
    "            tp += 1\n",
    "    \n",
    "    return tp\n",
    "\n",
    "\n",
    "def true_negative(y_true, y_pred):\n",
    "    \n",
    "    tn = 0\n",
    "    \n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        \n",
    "        if yt == 0 and yp == 0:\n",
    "            tn += 1\n",
    "            \n",
    "    return tn\n",
    "\n",
    "\n",
    "def false_positive(y_true, y_pred):\n",
    "    \n",
    "    fp = 0\n",
    "    \n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        \n",
    "        if yt == 1 and yp == 0: #01\n",
    "            fp += 1\n",
    "            \n",
    "    return fp\n",
    "\n",
    "\n",
    "def false_negative(y_true, y_pred):\n",
    "    \n",
    "    fn = 0\n",
    "    \n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        \n",
    "        if yt == 0 and yp == 1: #10\n",
    "            fn += 1\n",
    "            \n",
    "    return fn\n",
    "\n",
    "def macro_precision(y_true, y_pred):\n",
    "    \n",
    "    # find the number of classes\n",
    "    num_classes = len(np.unique(y_true))\n",
    "\n",
    "    # initialize precision to 0\n",
    "    precision = 0\n",
    "    \n",
    "    # loop over all classes\n",
    "    for class_ in list(np.unique(y_true)):\n",
    "        \n",
    "        # all classes except current are considered negative\n",
    "        temp_true = [1 if p == class_ else 0 for p in y_true]\n",
    "        temp_pred = [1 if p == class_ else 0 for p in y_pred]\n",
    "        \n",
    "        \n",
    "        # compute true positive for current class\n",
    "        tp = true_positive(temp_true, temp_pred)\n",
    "        \n",
    "        # compute false positive for current class\n",
    "        fp = false_positive(temp_true, temp_pred)\n",
    "        \n",
    "        \n",
    "        # compute precision for current class\n",
    "        temp_precision = tp / (tp + fp + 1e-6)\n",
    "        # keep adding precision for all classes\n",
    "        precision += temp_precision\n",
    "        \n",
    "    # calculate and return average precision over all classes\n",
    "    precision /= num_classes\n",
    "    \n",
    "    return precision\n",
    "\n",
    "\n",
    "def micro_precision(y_true, y_pred):\n",
    "    \n",
    "\n",
    "    # find the number of classes \n",
    "    num_classes = len(np.unique(y_true))\n",
    "    \n",
    "    # initialize tp and fp to 0\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    \n",
    "    # loop over all classes\n",
    "    for class_ in np.unique(y_true):\n",
    "        \n",
    "        # all classes except current are considered negative\n",
    "        temp_true = [1 if p == class_ else 0 for p in y_true]\n",
    "        temp_pred = [1 if p == class_ else 0 for p in y_pred]\n",
    "        \n",
    "        # calculate true positive for current class\n",
    "        # and update overall tp\n",
    "        tp += true_positive(temp_true, temp_pred)\n",
    "        \n",
    "        # calculate false positive for current class\n",
    "        # and update overall tp\n",
    "        fp += false_positive(temp_true, temp_pred)\n",
    "        \n",
    "    # calculate and return overall precision\n",
    "    precision = tp / (tp + fp)\n",
    "    return precision\n",
    "\n",
    "def macro_recall(y_true, y_pred):\n",
    "    \n",
    "    # find the number of classes\n",
    "    num_classes = len(np.unique(y_true))\n",
    "\n",
    "    # initialize recall to 0\n",
    "    recall = 0\n",
    "    \n",
    "    # loop over all classes\n",
    "    for class_ in list(np.unique(y_true)):\n",
    "        \n",
    "        # all classes except current are considered negative\n",
    "        temp_true = [1 if p == class_ else 0 for p in y_true]\n",
    "        temp_pred = [1 if p == class_ else 0 for p in y_pred]\n",
    "        \n",
    "        \n",
    "        # compute true positive for current class\n",
    "        tp = true_positive(temp_true, temp_pred)\n",
    "        \n",
    "        # compute false negative for current class\n",
    "        fn = false_negative(temp_true, temp_pred)\n",
    "        \n",
    "        \n",
    "        # compute recall for current class\n",
    "        temp_recall = tp / (tp + fn + 1e-6)\n",
    "        \n",
    "        # keep adding recall for all classes\n",
    "        recall += temp_recall\n",
    "        \n",
    "    # calculate and return average recall over all classes\n",
    "    recall /= num_classes\n",
    "    \n",
    "    return recall\n",
    "\n",
    "def micro_recall(y_true, y_pred):\n",
    "    \n",
    "\n",
    "    # find the number of classes \n",
    "    num_classes = len(np.unique(y_true))\n",
    "    \n",
    "    # initialize tp and fp to 0\n",
    "    tp = 0\n",
    "    fn = 0\n",
    "    \n",
    "    # loop over all classes\n",
    "    for class_ in np.unique(y_true):\n",
    "        \n",
    "        # all classes except current are considered negative\n",
    "        temp_true = [1 if p == class_ else 0 for p in y_true]\n",
    "        temp_pred = [1 if p == class_ else 0 for p in y_pred]\n",
    "        \n",
    "        # calculate true positive for current class\n",
    "        # and update overall tp\n",
    "        tp += true_positive(temp_true, temp_pred)\n",
    "        \n",
    "        # calculate false negative for current class\n",
    "        # and update overall tp\n",
    "        fn += false_negative(temp_true, temp_pred)\n",
    "        \n",
    "    # calculate and return overall recall\n",
    "    recall = tp / (tp + fn)\n",
    "    return recall\n",
    "\n",
    "def macro_f1(y_true, y_pred):\n",
    "    \n",
    "    # find the number of classes\n",
    "    num_classes = len(np.unique(y_true))\n",
    "\n",
    "    # initialize f1 to 0\n",
    "    f1 = 0\n",
    "    \n",
    "    # loop over all classes\n",
    "    for class_ in list(np.unique(y_true)):\n",
    "        \n",
    "        # all classes except current are considered negative\n",
    "        temp_true = [1 if p == class_ else 0 for p in y_true]\n",
    "        temp_pred = [1 if p == class_ else 0 for p in y_pred]\n",
    "        \n",
    "        \n",
    "        # compute true positive for current class\n",
    "        tp = true_positive(temp_true, temp_pred)\n",
    "        \n",
    "        # compute false negative for current class\n",
    "        fn = false_negative(temp_true, temp_pred)\n",
    "        \n",
    "        # compute false positive for current class\n",
    "        fp = false_positive(temp_true, temp_pred)\n",
    "\n",
    "        # compute recall for current class\n",
    "        temp_recall = tp / (tp + fn + 1e-6)\n",
    "        \n",
    "        # compute precision for current class\n",
    "        temp_precision = tp / (tp + fp + 1e-6)\n",
    "        \n",
    "        \n",
    "        temp_f1 = 2 * temp_precision * temp_recall / (temp_precision + temp_recall + 1e-6)\n",
    "        \n",
    "        # keep adding f1 score for all classes\n",
    "        f1 += temp_f1\n",
    "        \n",
    "    # calculate and return average f1 score over all classes\n",
    "    f1 /= num_classes\n",
    "    \n",
    "    return f1\n",
    "\n",
    "def micro_f1(y_true, y_pred):\n",
    "    \n",
    "\n",
    "    #micro-averaged precision score\n",
    "    P = micro_precision(y_true, y_pred)\n",
    "\n",
    "    #micro-averaged recall score\n",
    "    R = micro_recall(y_true, y_pred)\n",
    "\n",
    "    #micro averaged f1 score\n",
    "    f1 = 2*P*R / (P + R)    \n",
    "\n",
    "    return f1\n",
    "\n",
    "\n",
    "def roc_auc_score_multiclass(actual_class, pred_class, average = \"macro\"):\n",
    "    \n",
    "    #creating a set of all the unique classes using the actual class list\n",
    "    unique_class = set(actual_class)\n",
    "    roc_auc_dict = {}\n",
    "    for per_class in unique_class:\n",
    "        \n",
    "        #creating a list of all the classes except the current class \n",
    "        other_class = [x for x in unique_class if x != per_class]\n",
    "\n",
    "        #marking the current class as 1 and all other classes as 0\n",
    "        new_actual_class = [0 if x in other_class else 1 for x in actual_class]\n",
    "        new_pred_class = [0 if x in other_class else 1 for x in pred_class]\n",
    "\n",
    "        #using the sklearn metrics method to calculate the roc_auc_score\n",
    "        roc_auc = roc_auc_score(new_actual_class, new_pred_class, average = average)\n",
    "        roc_auc_dict[per_class] = roc_auc\n",
    "\n",
    "    return roc_auc_dict\n",
    "\n",
    "\n",
    "\n",
    "#3 - models_algoritms\n",
    "\n",
    "#MLP model\n",
    "def make_model(METRICS, train_features,output_bias=None):\n",
    "    if output_bias is not None:\n",
    "        output_bias = tf.keras.initializers.Constant(output_bias)\n",
    "    \n",
    "\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Dense(\n",
    "            16, activation='relu',\n",
    "            input_shape=(train_features.shape[-1],)),\n",
    "        keras.layers.Dropout(0.5),\n",
    "        keras.layers.Dense(1, activation='sigmoid',\n",
    "                            bias_initializer=output_bias),\n",
    "    ])\n",
    "\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss=keras.losses.BinaryCrossentropy(),\n",
    "        metrics=METRICS)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "#LSTM model\n",
    "def make_model_LSTM(METRICS, output_bias=None):\n",
    "    if output_bias is not None:\n",
    "        output_bias = tf.keras.initializers.Constant(output_bias)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(8,input_shape=(18,1),return_sequences=False))#True = many to many #numero colunas csv\n",
    "    model.add(Dense(2,kernel_initializer='normal',activation='linear'))\n",
    "    model.add(Dense(1,kernel_initializer='normal',activation='linear'))\n",
    "    model.compile(loss='mse',optimizer ='adam',metrics=METRICS)\n",
    "\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def make_model_SVM(METRICS,output_bias=None):\n",
    "    model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(18,)), #number of fields in my csv\n",
    "        RandomFourierFeatures(\n",
    "            output_dim=4096, scale=10.0, kernel_initializer=\"gaussian\"\n",
    "        ),\n",
    "        layers.Dense(units=1),\n",
    "    ]\n",
    "    )  \n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
    "        loss=keras.losses.hinge,\n",
    "        metrics=METRICS,\n",
    "    )\n",
    "        \n",
    "    return model\n",
    "\n",
    "\n",
    "#Transformers Model\n",
    "def transformer_model(input_shape, n_classes):\n",
    "    model = build_model_transformers(\n",
    "        n_classes,\n",
    "        input_shape,\n",
    "        head_size=256, #256\n",
    "        num_heads=2, #4\n",
    "        ff_dim=4, #4\n",
    "        num_transformer_blocks=4, #4\n",
    "        mlp_units=[128], #128\n",
    "        mlp_dropout=0.4, #0.4 \n",
    "        dropout=0.25,  #0.25\n",
    "    )   \n",
    "\n",
    "\n",
    "    loss_function = \"sparse_categorical_crossentropy\"\n",
    "\n",
    "    model.compile(\n",
    "        loss=loss_function,\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
    "        metrics = [\"accuracy\"],\n",
    "        )\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "#transformer-encoder\n",
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    # Normalization and Attention\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
    "    x = layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
    "    )(x, x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    res = x + inputs\n",
    "\n",
    "    # Feed Forward Part\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(res)\n",
    "    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
    "\n",
    "    return x + res\n",
    "\n",
    "#https://keras.io/examples/timeseries/timeseries_classification_transformer/\n",
    "def build_model_transformers(\n",
    "        n_classes,\n",
    "        input_shape,\n",
    "        head_size,\n",
    "        num_heads,\n",
    "        ff_dim,\n",
    "        num_transformer_blocks,\n",
    "        mlp_units,\n",
    "        dropout=0,\n",
    "        mlp_dropout=0,\n",
    "    ):\n",
    "        inputs = keras.Input(shape=input_shape)\n",
    "        x = inputs\n",
    "        for _ in range(num_transformer_blocks):\n",
    "            x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
    "\n",
    "        x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
    "        for dim in mlp_units:\n",
    "            x = layers.Dense(dim, activation=\"relu\")(x) \n",
    "            x = layers.Dropout(mlp_dropout)(x)\n",
    "            \n",
    "        \n",
    "        outputs = layers.Dense(n_classes, activation=\"softmax\")(x)\n",
    "        return keras.Model(inputs, outputs)\n",
    "\n",
    "\n",
    "#4 - Divide datasets and execute models\n",
    "\n",
    "def execute_all_models(set_dataset_base, set_window, set_batch, set_epoch, cleaned_df, set_model_name, set_balancing, set_dataset_division, date_chronological):\n",
    "\n",
    "\n",
    "    #4.1 divide datasets\n",
    "\n",
    "    if set_dataset_division == 'chronological': \n",
    "\n",
    "        datasets = date_chronological\n",
    "\n",
    "        #Datasets for solar cycle A1, A2, A3, A4\n",
    "        if len(datasets) > 6:\n",
    "            train_start_date_1 = datasets[0]\n",
    "            train_end_date_1 = datasets[1]\n",
    "\n",
    "            train_start_date_2 = datasets[2]\n",
    "            train_end_date_2 = datasets[3]\n",
    "\n",
    "            val_start_date = datasets[4]\n",
    "            val_end_date = datasets[5]\n",
    "\n",
    "            test_start_date = datasets[6]\n",
    "            test_end_date = datasets[7]\n",
    "\n",
    "            #create train sets\n",
    "            train_df_1 = cleaned_df.loc[(cleaned_df.T_REC >= train_start_date_1) & (cleaned_df.T_REC <= train_end_date_1)]\n",
    "            train_df_2 = cleaned_df.loc[(cleaned_df.T_REC >= train_start_date_2) & (cleaned_df.T_REC <= train_end_date_2)]\n",
    "            train_df = dd.concat([train_df_1, train_df_2])\n",
    "        #All dataset\n",
    "        else:\n",
    "            train_start_date = datasets[0]\n",
    "            train_end_date = datasets[1]\n",
    "\n",
    "            val_start_date = datasets[2]\n",
    "            val_end_date = datasets[3]\n",
    "\n",
    "            test_start_date = datasets[4]\n",
    "            test_end_date = datasets[5]\n",
    "\n",
    "            #create train sets\n",
    "            train_df = cleaned_df.loc[(cleaned_df.T_REC >= train_start_date) & (cleaned_df.T_REC <= train_end_date)]\n",
    "\n",
    "\n",
    "        #create val and test sets\n",
    "        val_df = cleaned_df.loc[(cleaned_df.T_REC >= val_start_date) & (cleaned_df.T_REC <= val_end_date)]\n",
    "        test_df = cleaned_df.loc[(cleaned_df.T_REC >= test_start_date) & (cleaned_df.T_REC <= test_end_date)]\n",
    "\n",
    "        #sort datasets by date\n",
    "        train_df = train_df.sort_values(by='T_REC')\n",
    "        val_df = val_df.sort_values(by='T_REC')\n",
    "        test_df = test_df.sort_values(by='T_REC')\n",
    "\n",
    "    elif set_dataset_division == 'random':\n",
    "\n",
    "        #separate positive and negative rows\n",
    "        df_neg, df_pos = cleaned_df[(mask:=cleaned_df['Class'] == 0)], cleaned_df[~mask]\n",
    "        \n",
    "        #splits sets randomly\n",
    "        train_df_pos, test_df_pos = df_pos.random_split([0.8, 0.2])  \n",
    "        train_df_pos, val_df_pos = train_df_pos.random_split([0.75, 0.25]) \n",
    "\n",
    "        train_df_neg, test_df_neg = df_neg.random_split([0.8, 0.2]) \n",
    "        train_df_neg, val_df_neg = train_df_neg.random_split([0.75, 0.25]) \n",
    "\n",
    "\n",
    "        train_df =  dd.concat([train_df_pos, train_df_neg])\n",
    "        val_df = dd.concat([val_df_pos,val_df_neg])\n",
    "        test_df = dd.concat([test_df_pos, test_df_neg])\n",
    "    \n",
    "\n",
    "        #order by date dataframe rows\n",
    "        train_df = train_df.sort_values(by='T_REC')\n",
    "        val_df = val_df.sort_values(by='T_REC')\n",
    "        test_df = test_df.sort_values(by='T_REC')\n",
    "        \n",
    "\n",
    "\n",
    "    #4.2 count train, val, test sets\n",
    "\n",
    "    neg_t = len(train_df [train_df.Class == 0])\n",
    "    pos_t = len(train_df [train_df.Class == 1])\n",
    "\n",
    "    print(\"\\n Train: neg=>\", neg_t, \" pos=>\", pos_t)\n",
    "\n",
    "\n",
    "    neg_v = len(val_df [val_df .Class == 0])\n",
    "    pos_v = len(val_df [val_df .Class == 1])\n",
    "\n",
    "    print(\"\\nVal: neg=>\", neg_v, \" pos=>\", pos_v)\n",
    "\n",
    "    neg_te = len(test_df [test_df .Class == 0])\n",
    "    pos_te = len(test_df [test_df .Class == 1])\n",
    "\n",
    "    print(\"\\nTest: neg=>\", neg_te, \" pos=>\", pos_te)\n",
    "\n",
    "    #4.3 Calculate class weights\n",
    "\n",
    "    neg = len(train_df [train_df.Class == 0])\n",
    "    pos = len(train_df [train_df.Class == 1])\n",
    "\n",
    "    total = pos  + neg\n",
    "    weight_for_0 = (1 / neg) * (total / 2.0)\n",
    "    weight_for_1 = (1 / pos) * (total / 2.0)\n",
    "\n",
    "    class_weight = {0: weight_for_0, 1: weight_for_1}\n",
    "\n",
    "    print('Weight class 0: {:.2f}'.format(weight_for_0))\n",
    "    print('Weight class 1: {:.2f}'.format(weight_for_1))\n",
    "    print(class_weight)\n",
    "\n",
    "\n",
    "    #4.4 Balancing training set\n",
    "\n",
    "    #SMOTE balancing\n",
    "    if set_balancing == 'smote':\n",
    "        \n",
    "        train_df['T_REC'] = train_df['T_REC'].apply(lambda x: x.value)\n",
    "        X = train_df.loc[:, train_df.columns != 'Class']\n",
    "        y = train_df.Class\n",
    "        sm = SMOTE(sampling_strategy='auto', k_neighbors=1, random_state=100)\n",
    "        X_res, y_res = sm.fit_resample(X, y)\n",
    "        \n",
    "        train_df = pd.concat([pd.DataFrame(X_res), pd.DataFrame(y_res)], axis=1)\n",
    "        \n",
    "        train_df['T_REC'] = train_df['T_REC'].apply(pd.Timestamp)\n",
    "\n",
    "        train_df = train_df.sort_values(by='T_REC')\n",
    "\n",
    "        train_df = dd.from_pandas(train_df, npartitions=10)\n",
    "\n",
    "    #Oversampling Balancing\n",
    "    elif set_balancing == 'oversampling':\n",
    "\n",
    "        #balancing train_df oversamspling\n",
    "        pos_flare = train_df[train_df['Class']==1]\n",
    "        neg_flare = train_df[train_df['Class']==0]\n",
    "\n",
    "        pos_flare = pos_flare.sample(frac = len(neg_flare) / len(pos_flare), replace = True, random_state=101)\n",
    "        train_df = dd.concat([pos_flare, neg_flare], interleave_partitions=True)   \n",
    "        train_df.sort_values(by='T_REC')\n",
    "          \n",
    "    #Undersampling Balancing\n",
    "    elif set_balancing == 'undersampling':\n",
    "     \n",
    "        pos_flare = train_df[train_df['Class']==1]\n",
    "        neg_flare = train_df[train_df['Class']==0]\n",
    "        \n",
    "        print(len(neg_flare), - len(pos_flare))\n",
    "\n",
    "        neg_flare = neg_flare.sample(frac= 1 / ( len(neg_flare)/len(pos_flare)) , random_state=101)\n",
    "        train_df = dd.concat([pos_flare, neg_flare], interleave_partitions=True)   \n",
    "        train_df.sort_values(by='T_REC')\n",
    "\n",
    "\n",
    "    \n",
    "    #4.5 Count Class after balancing\n",
    "    pos = len(train_df[train_df['Class']==1])\n",
    "    neg = len(train_df[train_df['Class']==0])\n",
    "\n",
    "    total = neg + pos\n",
    "        \n",
    "    print('After balancing - Negative:{} ({:.2f}% of total)\\n'.format(neg, 100 * neg / total))\n",
    "    print('After balancing - Positive: {} ({:.2f}% of total)\\n'.format(pos, 100 * pos / total))\n",
    "    \n",
    "    \n",
    "    #4.6 Clean keys before training\n",
    "    train_df.pop('T_REC')\n",
    "    val_df.pop('T_REC')\n",
    "    test_df.pop('T_REC')\n",
    "\n",
    "    train_df.pop('harpnum')\n",
    "    val_df.pop('harpnum')\n",
    "    test_df.pop('harpnum')\n",
    "\n",
    "    \n",
    "    #4.7 Form np arrays of labels and features.\n",
    "    train_labels = np.array(train_df.pop('Class'))\n",
    "    bool_train_labels = train_labels != 0\n",
    "    val_labels = np.array(val_df.pop('Class'))\n",
    "    test_labels = np.array(test_df.pop('Class'))\n",
    "\n",
    "\n",
    "    #4.8 Create features from each dataset\n",
    "    train_features = np.array(train_df)\n",
    "    val_features = np.array(val_df)\n",
    "    test_features = np.array(test_df)\n",
    "\n",
    "    #4.9 Dataset normalization - StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    train_features = scaler.fit_transform(train_features)\n",
    "    val_features = scaler.transform(val_features)\n",
    "    test_features = scaler.transform(test_features)\n",
    "\n",
    "    train_features = np.clip(train_features, -5, 5)\n",
    "    val_features = np.clip(val_features, -5, 5)\n",
    "    test_features = np.clip(test_features, -5, 5)\n",
    "\n",
    "    #4.10 Set variables model\n",
    "    if set_model_name == \"transformers\" :\n",
    "    \n",
    "        x_train  = train_features\n",
    "        x_val = val_features\n",
    "        x_test = test_features\n",
    "\n",
    "        y_train  = train_labels\n",
    "        y_val = val_labels\n",
    "        y_test = test_labels\n",
    "\n",
    "        y_train = y_train.astype(int)\n",
    "        y_val = y_val.astype(int)\n",
    "        y_test = y_test.astype(int)\n",
    "\n",
    "\n",
    "        n_classes = len(np.unique(y_train))\n",
    "\n",
    "        x_train = x_train.reshape((x_train.shape[0], x_train.shape[1], 1))\n",
    "        x_val = x_val.reshape((x_val.shape[0], x_val.shape[1], 1))\n",
    "        x_test = x_test.reshape((x_test.shape[0], x_test.shape[1], 1))\n",
    "\n",
    "\n",
    "        idx = np.random.permutation(len(x_train))\n",
    "        x_train = x_train[idx]\n",
    "        y_train = y_train[idx]\n",
    "\n",
    "\n",
    "        input_shape = x_train.shape[1:]\n",
    "   \n",
    "        #Create and Compile Model\n",
    "        if set_model_name == \"transformers\":\n",
    "            model = transformer_model(input_shape, n_classes)\n",
    "            model.summary()\n",
    "        else:\n",
    "            model = make_model_SVM()\n",
    "\n",
    "    else:\n",
    "        METRICS = [\"accuracy\"]\n",
    "        if set_model_name == \"mlp\":\n",
    "            model = make_model(METRICS,train_features)\n",
    "        elif set_model_name == \"lstm\":\n",
    "            model = make_model_LSTM(METRICS)\n",
    "        elif set_model_name == \"svm\":\n",
    "            model = make_model_SVM(METRICS)\n",
    "\n",
    "\n",
    "    #4.11 Model Fit\n",
    "    callbacks = [keras.callbacks.ModelCheckpoint(\"fraud_model_at_epoch_{epoch}.keras\")]\n",
    "\n",
    "    if set_model_name == \"transformers\": # or set_model_name == \"svm\" :\n",
    "\n",
    "        #Check use class weight\n",
    "        if set_balancing == 'weight':\n",
    "            model.fit(\n",
    "                x_train,\n",
    "                y_train,\n",
    "                validation_data=(x_val, y_val),\n",
    "                epochs=set_epoch,\n",
    "                batch_size=set_batch,\n",
    "                verbose = 1,\n",
    "                callbacks=callbacks,\n",
    "                class_weight = class_weight,   \n",
    "            )\n",
    "        else:\n",
    "            model.fit(\n",
    "            x_train,\n",
    "            y_train,\n",
    "            validation_data=(x_val, y_val),\n",
    "            epochs=set_epoch,\n",
    "            batch_size=set_batch,\n",
    "            verbose = 1,\n",
    "            callbacks=callbacks,\n",
    "            )\n",
    "\n",
    "    else:\n",
    "        if set_balancing == \"weight\":\n",
    "            model.fit(\n",
    "                    train_features,\n",
    "                    train_labels,\n",
    "                    batch_size=set_batch,\n",
    "                    epochs=set_epoch,\n",
    "                    callbacks= callbacks,\n",
    "                    validation_data=(val_features, val_labels),\n",
    "                    class_weight=class_weight)\n",
    "        else:\n",
    "            model.fit(\n",
    "            train_features,\n",
    "            train_labels,\n",
    "            batch_size=set_batch,\n",
    "            epochs=set_epoch,\n",
    "            callbacks= callbacks,\n",
    "            validation_data=(val_features, val_labels),\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #4.12 Model Evaluate \n",
    "    if set_model_name == \"transformers\": \n",
    "        x_orig = x_test\n",
    "        y_orig = y_test\n",
    "    else:\n",
    "        x_orig = test_features\n",
    "        y_orig = test_labels\n",
    "\n",
    "    SCORE = model.evaluate(x_orig, y_orig, verbose=1)\n",
    "    print('Score:', SCORE)\n",
    "\n",
    "    #get loss metric\n",
    "    loss = -1\n",
    "    for name, value in zip(model.metrics_names, SCORE):\n",
    "        if name == \"loss\":\n",
    "            loss = value\n",
    "\n",
    "\n",
    "    #4.13 Model Predict\n",
    "\n",
    "    y_pred = (model.predict(x_orig) > 0.5).astype(\"int32\")\n",
    "\n",
    "\n",
    "    #Create val predict\n",
    "    class_predict_2 = []\n",
    "    cont_0 = 0\n",
    "    cont_1 = 0\n",
    "    count_tudo = 0\n",
    "\n",
    "\n",
    "    if set_model_name == \"transformers\": \n",
    "        for cp in y_pred:\n",
    "            count_tudo += 1\n",
    "            if cp[0] == 1:\n",
    "                class_predict_2.append(0)\n",
    "                cont_0 = cont_0 + 1\n",
    "            elif cp[1] == 1:\n",
    "                class_predict_2.append(1)\n",
    "                cont_1 = cont_1 + 1\n",
    "            else:\n",
    "                class_predict_2.append(0)\n",
    "    else:\n",
    "        for cp in y_pred:\n",
    "            if cp == 1:\n",
    "                class_predict_2.append(1)\n",
    "            else:\n",
    "                class_predict_2.append(0)\n",
    "\n",
    "    y_pred = class_predict_2\n",
    "\n",
    "  \n",
    "\n",
    "    #4.14 Show confusion matrix and results\n",
    "\n",
    "    labels = [\"NEGATIVE\", \"POSITIVE\"]\n",
    "    cm = confusion_matrix(y_orig, y_pred)\n",
    "\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "\n",
    "    disp.plot()\n",
    "\n",
    "    #4.15 Get metrics' results \n",
    "\n",
    "    tp = true_positive(y_orig, y_pred)\n",
    "    fp = false_positive(y_orig, y_pred)\n",
    "    tn = true_negative(y_orig, y_pred)\n",
    "    fn = false_negative(y_orig, y_pred)\n",
    "\n",
    "    val_TSS = TSS_matrix_confusion(tn, fp, fn, tp)\n",
    "    val_HSS = HSS_matrix_confusion(tn, fp, fn, tp)\n",
    "    val_FAR = FAR_matrix_confusion(tn, fp, fn, tp)\n",
    "\n",
    "    print(\"TP: \", tp)\n",
    "    print(\"TN \", tn)\n",
    "    print(\"FP \", fp) \n",
    "    print(\"FN: \", fn) \n",
    "\n",
    "\n",
    "    print(\"TSS: \", val_TSS)\n",
    "    print(\"HSS: \", val_HSS)\n",
    "    print(\"FAR: \", val_FAR)\n",
    "    print(\"LOSS:\",loss)\n",
    "\n",
    "    print(\"ROC AUC: \", roc_auc_score_multiclass(y_orig, y_pred)[1])\n",
    "    print(\"Accuracy: \", accuracy(y_orig, y_pred))\n",
    "    print(\"Macro precision: \", macro_precision(y_orig, y_pred))\n",
    "    print(\"Micro precision: \", micro_precision(y_orig, y_pred))\n",
    "    print(\"Macro Recall: \", macro_recall(y_orig, y_pred))\n",
    "    print(\"Micro Recall: \", micro_recall(y_orig, y_pred))\n",
    "    print(\"Macro F1: \", macro_f1(y_orig, y_pred))\n",
    "    print(\"Micro F1: \", micro_f1(y_orig, y_pred))\n",
    "\n",
    "\n",
    "    #4.16 - Salve metrics' results in .csv file\n",
    "    metric_list = []\n",
    "    metric_list.append(set_dataset_base)\n",
    "    metric_list.append(set_window)\n",
    "    metric_list.append(set_model_name)\n",
    "    metric_list.append(set_balancing)\n",
    "    metric_list.append(set_dataset_division)\n",
    "    metric_list.append(set_batch)\n",
    "    metric_list.append(set_epoch)\n",
    "    metric_list.append(tp)\n",
    "    metric_list.append(tn)\n",
    "    metric_list.append(fp)\n",
    "    metric_list.append(fn)\n",
    "    metric_list.append(accuracy(y_orig, y_pred))\n",
    "    metric_list.append(val_TSS)\n",
    "    metric_list.append(val_HSS)\n",
    "    metric_list.append(roc_auc_score_multiclass(y_orig, y_pred)[1])\n",
    "    metric_list.append(val_FAR)\n",
    "    metric_list.append(loss)\n",
    "    metric_list.append(macro_precision(y_orig, y_pred))\n",
    "    metric_list.append(micro_precision(y_orig, y_pred))\n",
    "    metric_list.append(macro_recall(y_orig, y_pred))\n",
    "    metric_list.append(micro_recall(y_orig, y_pred))\n",
    "    metric_list.append(macro_f1(y_orig, y_pred))\n",
    "    metric_list.append(micro_f1(y_orig, y_pred))\n",
    "\n",
    "    \n",
    "    df_results = pd.DataFrame(metric_list).T\n",
    "    df_results.columns = [\"dataset\", \"window\", \"model\", \"balancing\", \"sep_datasets\", \"batch\", \"epoch\", \"TP\", \"TN\", \"FP\", \"FN\", \"Acurácia\", \"TSS\" ,\"HSS\" ,\"AUC/ROC\" ,\"FAR\" ,\"LOSS\" ,\"Macro Precision\" ,\"Micro Precision\" ,\"Macro Recall\" ,\"Micro Recall\" ,\"Macro F1\" ,\"Micro F1\"]\n",
    "\n",
    "    if os.path.isfile('data/result_models_test.csv'):\n",
    "        df_results.to_csv('data/result_models_test.csv', index= False, header=False, mode = 'a')\n",
    "    else:\n",
    "        df_results.to_csv('data/result_models_test.csv', index= False, header=True, mode = 'a')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lenght before drop nan values:  118566\n",
      "Lenght after drop nan values:  117066\n",
      "A1  -  24h  -  64  -  10  -  transformers  -  weight  - \n",
      "\n",
      " Train: neg=> 8435  pos=> 73\n",
      "\n",
      "Val: neg=> 1209  pos=> 4\n",
      "\n",
      "Test: neg=> 2365  pos=> 26\n",
      "Weight class 0: 0.50\n",
      "Weight class 1: 58.27\n",
      "{0: 0.5043272080616479, 1: 58.273972602739725}\n",
      "After balancing - Negative:8435 (99.14% of total)\n",
      "\n",
      "After balancing - Positive: 73 (0.86% of total)\n",
      "\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_47 (InputLayer)          [(None, 18, 1)]      0           []                               \n",
      "                                                                                                  \n",
      " layer_normalization_16 (LayerN  (None, 18, 1)       2           ['input_47[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_8 (MultiH  (None, 18, 1)       3585        ['layer_normalization_16[0][0]', \n",
      " eadAttention)                                                    'layer_normalization_16[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_18 (Dropout)           (None, 18, 1)        0           ['multi_head_attention_8[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_16 (TFOpL  (None, 18, 1)       0           ['dropout_18[0][0]',             \n",
      " ambda)                                                           'input_47[0][0]']               \n",
      "                                                                                                  \n",
      " layer_normalization_17 (LayerN  (None, 18, 1)       2           ['tf.__operators__.add_16[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv1d_16 (Conv1D)             (None, 18, 4)        8           ['layer_normalization_17[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_19 (Dropout)           (None, 18, 4)        0           ['conv1d_16[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_17 (Conv1D)             (None, 18, 1)        5           ['dropout_19[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_17 (TFOpL  (None, 18, 1)       0           ['conv1d_17[0][0]',              \n",
      " ambda)                                                           'tf.__operators__.add_16[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_18 (LayerN  (None, 18, 1)       2           ['tf.__operators__.add_17[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_9 (MultiH  (None, 18, 1)       3585        ['layer_normalization_18[0][0]', \n",
      " eadAttention)                                                    'layer_normalization_18[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_20 (Dropout)           (None, 18, 1)        0           ['multi_head_attention_9[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_18 (TFOpL  (None, 18, 1)       0           ['dropout_20[0][0]',             \n",
      " ambda)                                                           'tf.__operators__.add_17[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_19 (LayerN  (None, 18, 1)       2           ['tf.__operators__.add_18[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv1d_18 (Conv1D)             (None, 18, 4)        8           ['layer_normalization_19[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_21 (Dropout)           (None, 18, 4)        0           ['conv1d_18[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_19 (Conv1D)             (None, 18, 1)        5           ['dropout_21[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_19 (TFOpL  (None, 18, 1)       0           ['conv1d_19[0][0]',              \n",
      " ambda)                                                           'tf.__operators__.add_18[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_20 (LayerN  (None, 18, 1)       2           ['tf.__operators__.add_19[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_10 (Multi  (None, 18, 1)       3585        ['layer_normalization_20[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_20[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_22 (Dropout)           (None, 18, 1)        0           ['multi_head_attention_10[0][0]']\n",
      "                                                                                                  \n",
      " tf.__operators__.add_20 (TFOpL  (None, 18, 1)       0           ['dropout_22[0][0]',             \n",
      " ambda)                                                           'tf.__operators__.add_19[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_21 (LayerN  (None, 18, 1)       2           ['tf.__operators__.add_20[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv1d_20 (Conv1D)             (None, 18, 4)        8           ['layer_normalization_21[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_23 (Dropout)           (None, 18, 4)        0           ['conv1d_20[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_21 (Conv1D)             (None, 18, 1)        5           ['dropout_23[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_21 (TFOpL  (None, 18, 1)       0           ['conv1d_21[0][0]',              \n",
      " ambda)                                                           'tf.__operators__.add_20[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_22 (LayerN  (None, 18, 1)       2           ['tf.__operators__.add_21[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_11 (Multi  (None, 18, 1)       3585        ['layer_normalization_22[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_22[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_24 (Dropout)           (None, 18, 1)        0           ['multi_head_attention_11[0][0]']\n",
      "                                                                                                  \n",
      " tf.__operators__.add_22 (TFOpL  (None, 18, 1)       0           ['dropout_24[0][0]',             \n",
      " ambda)                                                           'tf.__operators__.add_21[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_23 (LayerN  (None, 18, 1)       2           ['tf.__operators__.add_22[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv1d_22 (Conv1D)             (None, 18, 4)        8           ['layer_normalization_23[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_25 (Dropout)           (None, 18, 4)        0           ['conv1d_22[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_23 (Conv1D)             (None, 18, 1)        5           ['dropout_25[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_23 (TFOpL  (None, 18, 1)       0           ['conv1d_23[0][0]',              \n",
      " ambda)                                                           'tf.__operators__.add_22[0][0]']\n",
      "                                                                                                  \n",
      " global_average_pooling1d_2 (Gl  (None, 18)          0           ['tf.__operators__.add_23[0][0]']\n",
      " obalAveragePooling1D)                                                                            \n",
      "                                                                                                  \n",
      " dense_48 (Dense)               (None, 128)          2432        ['global_average_pooling1d_2[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dropout_26 (Dropout)           (None, 128)          0           ['dense_48[0][0]']               \n",
      "                                                                                                  \n",
      " dense_49 (Dense)               (None, 2)            258         ['dropout_26[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 17,098\n",
      "Trainable params: 17,098\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/10\n",
      "133/133 [==============================] - 20s 122ms/step - loss: 0.4962 - accuracy: 0.8453 - val_loss: 0.2776 - val_accuracy: 0.9126\n",
      "Epoch 2/10\n",
      "133/133 [==============================] - 16s 123ms/step - loss: 0.3684 - accuracy: 0.8869 - val_loss: 0.2285 - val_accuracy: 0.9101\n",
      "Epoch 3/10\n",
      "133/133 [==============================] - 16s 122ms/step - loss: 0.3391 - accuracy: 0.8655 - val_loss: 0.2292 - val_accuracy: 0.8879\n",
      "Epoch 4/10\n",
      "133/133 [==============================] - 16s 122ms/step - loss: 0.3149 - accuracy: 0.8749 - val_loss: 0.2091 - val_accuracy: 0.9052\n",
      "Epoch 5/10\n",
      "133/133 [==============================] - 16s 120ms/step - loss: 0.3288 - accuracy: 0.8591 - val_loss: 0.2031 - val_accuracy: 0.9044\n",
      "Epoch 6/10\n",
      "133/133 [==============================] - 16s 121ms/step - loss: 0.3290 - accuracy: 0.8744 - val_loss: 0.2350 - val_accuracy: 0.8648\n",
      "Epoch 7/10\n",
      "133/133 [==============================] - 16s 120ms/step - loss: 0.3050 - accuracy: 0.8635 - val_loss: 0.1966 - val_accuracy: 0.8978\n",
      "Epoch 8/10\n",
      "133/133 [==============================] - 17s 128ms/step - loss: 0.2885 - accuracy: 0.8654 - val_loss: 0.2179 - val_accuracy: 0.8747\n",
      "Epoch 9/10\n",
      "133/133 [==============================] - 17s 127ms/step - loss: 0.2878 - accuracy: 0.8669 - val_loss: 0.1945 - val_accuracy: 0.8945\n",
      "Epoch 10/10\n",
      "133/133 [==============================] - 18s 133ms/step - loss: 0.3025 - accuracy: 0.8756 - val_loss: 0.2117 - val_accuracy: 0.8747\n",
      "75/75 [==============================] - 2s 26ms/step - loss: 0.3720 - accuracy: 0.8641\n",
      "Score: [0.3720090091228485, 0.864073634147644]\n",
      "75/75 [==============================] - 2s 24ms/step\n",
      "TP:  26\n",
      "TN  2040\n",
      "FP  0\n",
      "FN:  325\n",
      "TSS:  0.8625792811839323\n",
      "HSS:  0.06860890402320595\n",
      "FAR:  0.0\n",
      "LOSS: 0.3720090091228485\n",
      "ROC AUC:  0.9312896405919662\n",
      "Accuracy:  0.8640736093684651\n",
      "Macro precision:  0.9312896211788342\n",
      "Micro precision:  0.8640736093684651\n",
      "Macro Recall:  0.5370370366864203\n",
      "Micro Recall:  0.8640736093684651\n",
      "Macro F1:  0.5320753059736703\n",
      "Micro F1:  0.8640736093684651\n",
      "A1  -  24h  -  64  -  10  -  transformers  -  oversampling  - \n",
      "\n",
      " Train: neg=> 8435  pos=> 73\n",
      "\n",
      "Val: neg=> 1209  pos=> 4\n",
      "\n",
      "Test: neg=> 2365  pos=> 26\n",
      "Weight class 0: 0.50\n",
      "Weight class 1: 58.27\n",
      "{0: 0.5043272080616479, 1: 58.273972602739725}\n",
      "After balancing - Negative:8435 (50.00% of total)\n",
      "\n",
      "After balancing - Positive: 8436 (50.00% of total)\n",
      "\n",
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_48 (InputLayer)          [(None, 18, 1)]      0           []                               \n",
      "                                                                                                  \n",
      " layer_normalization_24 (LayerN  (None, 18, 1)       2           ['input_48[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_12 (Multi  (None, 18, 1)       3585        ['layer_normalization_24[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_24[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_27 (Dropout)           (None, 18, 1)        0           ['multi_head_attention_12[0][0]']\n",
      "                                                                                                  \n",
      " tf.__operators__.add_24 (TFOpL  (None, 18, 1)       0           ['dropout_27[0][0]',             \n",
      " ambda)                                                           'input_48[0][0]']               \n",
      "                                                                                                  \n",
      " layer_normalization_25 (LayerN  (None, 18, 1)       2           ['tf.__operators__.add_24[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv1d_24 (Conv1D)             (None, 18, 4)        8           ['layer_normalization_25[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_28 (Dropout)           (None, 18, 4)        0           ['conv1d_24[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_25 (Conv1D)             (None, 18, 1)        5           ['dropout_28[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_25 (TFOpL  (None, 18, 1)       0           ['conv1d_25[0][0]',              \n",
      " ambda)                                                           'tf.__operators__.add_24[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_26 (LayerN  (None, 18, 1)       2           ['tf.__operators__.add_25[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_13 (Multi  (None, 18, 1)       3585        ['layer_normalization_26[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_26[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_29 (Dropout)           (None, 18, 1)        0           ['multi_head_attention_13[0][0]']\n",
      "                                                                                                  \n",
      " tf.__operators__.add_26 (TFOpL  (None, 18, 1)       0           ['dropout_29[0][0]',             \n",
      " ambda)                                                           'tf.__operators__.add_25[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_27 (LayerN  (None, 18, 1)       2           ['tf.__operators__.add_26[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv1d_26 (Conv1D)             (None, 18, 4)        8           ['layer_normalization_27[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_30 (Dropout)           (None, 18, 4)        0           ['conv1d_26[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_27 (Conv1D)             (None, 18, 1)        5           ['dropout_30[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_27 (TFOpL  (None, 18, 1)       0           ['conv1d_27[0][0]',              \n",
      " ambda)                                                           'tf.__operators__.add_26[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_28 (LayerN  (None, 18, 1)       2           ['tf.__operators__.add_27[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_14 (Multi  (None, 18, 1)       3585        ['layer_normalization_28[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_28[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_31 (Dropout)           (None, 18, 1)        0           ['multi_head_attention_14[0][0]']\n",
      "                                                                                                  \n",
      " tf.__operators__.add_28 (TFOpL  (None, 18, 1)       0           ['dropout_31[0][0]',             \n",
      " ambda)                                                           'tf.__operators__.add_27[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_29 (LayerN  (None, 18, 1)       2           ['tf.__operators__.add_28[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv1d_28 (Conv1D)             (None, 18, 4)        8           ['layer_normalization_29[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_32 (Dropout)           (None, 18, 4)        0           ['conv1d_28[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_29 (Conv1D)             (None, 18, 1)        5           ['dropout_32[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_29 (TFOpL  (None, 18, 1)       0           ['conv1d_29[0][0]',              \n",
      " ambda)                                                           'tf.__operators__.add_28[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_30 (LayerN  (None, 18, 1)       2           ['tf.__operators__.add_29[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_15 (Multi  (None, 18, 1)       3585        ['layer_normalization_30[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_30[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_33 (Dropout)           (None, 18, 1)        0           ['multi_head_attention_15[0][0]']\n",
      "                                                                                                  \n",
      " tf.__operators__.add_30 (TFOpL  (None, 18, 1)       0           ['dropout_33[0][0]',             \n",
      " ambda)                                                           'tf.__operators__.add_29[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_31 (LayerN  (None, 18, 1)       2           ['tf.__operators__.add_30[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv1d_30 (Conv1D)             (None, 18, 4)        8           ['layer_normalization_31[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_34 (Dropout)           (None, 18, 4)        0           ['conv1d_30[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_31 (Conv1D)             (None, 18, 1)        5           ['dropout_34[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_31 (TFOpL  (None, 18, 1)       0           ['conv1d_31[0][0]',              \n",
      " ambda)                                                           'tf.__operators__.add_30[0][0]']\n",
      "                                                                                                  \n",
      " global_average_pooling1d_3 (Gl  (None, 18)          0           ['tf.__operators__.add_31[0][0]']\n",
      " obalAveragePooling1D)                                                                            \n",
      "                                                                                                  \n",
      " dense_50 (Dense)               (None, 128)          2432        ['global_average_pooling1d_3[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dropout_35 (Dropout)           (None, 128)          0           ['dense_50[0][0]']               \n",
      "                                                                                                  \n",
      " dense_51 (Dense)               (None, 2)            258         ['dropout_35[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 17,098\n",
      "Trainable params: 17,098\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/10\n",
      "264/264 [==============================] - 37s 128ms/step - loss: 0.3964 - accuracy: 0.8217 - val_loss: 0.2580 - val_accuracy: 0.8805\n",
      "Epoch 2/10\n",
      "264/264 [==============================] - 36s 136ms/step - loss: 0.2952 - accuracy: 0.8749 - val_loss: 0.2044 - val_accuracy: 0.9085\n",
      "Epoch 3/10\n",
      "264/264 [==============================] - 33s 126ms/step - loss: 0.2691 - accuracy: 0.8887 - val_loss: 0.2014 - val_accuracy: 0.8978\n",
      "Epoch 4/10\n",
      "264/264 [==============================] - 30s 113ms/step - loss: 0.2534 - accuracy: 0.8983 - val_loss: 0.1830 - val_accuracy: 0.9093\n",
      "Epoch 5/10\n",
      "264/264 [==============================] - 30s 114ms/step - loss: 0.2418 - accuracy: 0.9052 - val_loss: 0.1930 - val_accuracy: 0.8945\n",
      "Epoch 6/10\n",
      "264/264 [==============================] - 31s 117ms/step - loss: 0.2327 - accuracy: 0.9069 - val_loss: 0.1830 - val_accuracy: 0.9052\n",
      "Epoch 7/10\n",
      "264/264 [==============================] - 30s 114ms/step - loss: 0.2245 - accuracy: 0.9094 - val_loss: 0.1755 - val_accuracy: 0.9101\n",
      "Epoch 8/10\n",
      "264/264 [==============================] - 30s 112ms/step - loss: 0.2175 - accuracy: 0.9141 - val_loss: 0.1653 - val_accuracy: 0.9242\n",
      "Epoch 9/10\n",
      "264/264 [==============================] - 30s 115ms/step - loss: 0.2127 - accuracy: 0.9181 - val_loss: 0.1676 - val_accuracy: 0.9159\n",
      "Epoch 10/10\n",
      "264/264 [==============================] - 30s 113ms/step - loss: 0.2073 - accuracy: 0.9202 - val_loss: 0.1556 - val_accuracy: 0.9299\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.2779 - accuracy: 0.8729\n",
      "Score: [0.2779320776462555, 0.8728565573692322]\n",
      "75/75 [==============================] - 2s 24ms/step\n",
      "TP:  25\n",
      "TN  2062\n",
      "FP  1\n",
      "FN:  303\n",
      "TSS:  0.8334200683037891\n",
      "HSS:  0.07056949126401833\n",
      "FAR:  0.038461538461538464\n",
      "LOSS: 0.2779320776462555\n",
      "ROC AUC:  0.9167100341518947\n",
      "Accuracy:  0.8728565453785028\n",
      "Macro precision:  0.9167100154764409\n",
      "Micro precision:  0.8728565453785028\n",
      "Macro Recall:  0.5378673902519701\n",
      "Micro Recall:  0.8728565453785028\n",
      "Macro F1:  0.5362941414915872\n",
      "Micro F1:  0.8728565453785029\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "#set variables\n",
    "\n",
    "date_chronological_vazio = []\n",
    "#date_chronological = [\"2010-01-01 00:00:00\", \"2014-10-26 23:59:00\", \"2014-10-27 00:00:00\", \"2015-10-30 23:59:00\", \"2015-10-31 00:00:00\", \"2023-12-31 23:59:00\"]\n",
    "#date_chronological = [\"2010-01-01 00:00:00\", \"2017-12-31 23:59:00\", \"2018-01-01 00:00:00\", \"2020-12-31 23:59:00\", \"2021-01-01 00:00:00\", \"2023-12-31 23:59:00\"]\n",
    "\n",
    "\n",
    "#date A1 - 2010-2011\n",
    "date_chronological_A1 = [\"2010-05-03 00:00:00\", \"2010-10-31 23:59:59\", \"2011-06-01 00:00:00\", \"2011-12-31 23:59:59\", \"2010-11-01 00:00:00\", \"2011-01-31 23:59:59\", \"2011-02-21 00:00:00\", \"2011-05-31 23:59:59\"]\n",
    "\n",
    "#date A2 - 2012 - 2013\n",
    "date_chronological_A2 = [\"2012-01-01 00:00:00\", \"2012-08-31 23:59:59\", \"2013-07-01 00:00:00\", \"2014-08-31 23:59:59\", \"2012-09-01 00:00:00\", \"2013-01-31 23:59:59\", \"2013-02-01 00:00:00\", \"2013-06-30 23:59:59\"]\n",
    "\n",
    "#date A3  - 2014-2016\n",
    "date_chronological_A3 = [\"2014-09-01 00:00:00\", \"2015-03-31 23:59:59\", \"2015-10-01 00:00:00\", \"2016-06-30 23:59:59\", \"2015-04-01 00:00:00\", \"2015-06-30 23:59:59\", \"2015-07-01 00:00:00\", \"2015-09-30 23:59:59\"]\n",
    "\n",
    "#date A4 - 2016-2018\n",
    "date_chronological_A4 = [\"2016-07-01 00:00:00\", \"2017-02-28 23:59:59\", \"2017-09-01 00:00:00\", \"2018-07-31 23:59:59\", \"2017-03-01 00:00:00\", \"2017-05-31 23:59:59\", \"2017-06-01 00:00:00\", \"2017-08-31 23:59:59\"]\n",
    "\n",
    "#path dataset\n",
    "separator = ','\n",
    "url_data = 'data/sharp_abcmx-nf-all_6h_2010-01-01_2024-04-30.csv'\n",
    "\n",
    "#Coluns Delete in CSV file\n",
    "list_col_delete = ['DATE','DATE_S','DATE_B','DATE__OBS','DATE-OBS','T_OBS','OBS_VR', 'QUALITY', 'Class_Flare']\n",
    "\n",
    "#config datetime format    \n",
    "file = tf.keras.utils \n",
    "#custom_date_parser = lambda x: datetime.strptime(x, \"%Y.%m.%d_%H:%M:%S_TAI\") \n",
    "custom_date_parser = lambda x: datetime.strptime(x, '%Y-%m-%d %H:%M:%S') \n",
    " \n",
    "#load csv \n",
    "raw_df = pd.read_csv(url_data, sep=',')\n",
    "raw_df['T_REC'] = pd.to_datetime(raw_df['T_REC'], format='%Y-%m-%d %H:%M:%S')\n",
    "raw_df.sort_values(by='T_REC')\n",
    "\n",
    "#drop nan values\n",
    "print(\"Lenght before drop nan values: \", len(raw_df))\n",
    "raw_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "raw_df.dropna(subset=raw_df.columns, inplace=True)\n",
    "raw_df.drop\n",
    "print(\"Lenght after drop nan values: \", len(raw_df))\n",
    "\n",
    "\n",
    "#convert pandas' dataset to dask dataframe\n",
    "cleaned_df = dd.from_pandas(raw_df, npartitions=10)\n",
    "\n",
    "\n",
    "for lcd in list_col_delete:\n",
    "    cleaned_df.pop(lcd)\n",
    "\n",
    "\n",
    "#Attributes to exclude (reduction of 10 attributes)\n",
    "'''\n",
    "cleaned_df.pop('MEANSHR')\n",
    "cleaned_df.pop('MEANGAM')\n",
    "cleaned_df.pop('MEANGBT')\n",
    "cleaned_df.pop('MEANGBZ')\n",
    "cleaned_df.pop('MEANGBH')\n",
    "cleaned_df.pop('MEANJZH')\n",
    "cleaned_df.pop('MEANJZD')\n",
    "cleaned_df.pop('MEANALP')\n",
    "'''\n",
    "\n",
    "\n",
    "#Attributes to exclude (reduction of 10 attributes)\n",
    "neg = len(cleaned_df [cleaned_df.Class == 0])\n",
    "pos = len(cleaned_df [cleaned_df.Class == 1])\n",
    "\n",
    "\n",
    "#set parameters before training\n",
    "set_dataset_base = \"all-abcmx\"\n",
    "set_window = \"24h\"\n",
    "set_epoch  = 10\n",
    "set_batch = 64\n",
    "set_model_name = ['transformers'] # ['mlp', 'svm','lstm','transformers']\n",
    "set_dataset_base = ['A1'] #['all', 'A1', 'A2', 'A3', 'A4']\n",
    "set_balancing = ['weight', 'oversampling'] # ['weight', 'undersampling', 'oversampling', 'smote']\n",
    "\n",
    "\n",
    "#training models\n",
    "for smn in set_model_name:\n",
    "\n",
    "    for sdb in set_dataset_base:\n",
    "\n",
    "        for sba in set_balancing:\n",
    "            print(sdb, ' - ', set_window,  ' - ', set_batch,  ' - ', set_epoch, ' - ', smn,  ' - ', sba,  ' - ')\n",
    "            if sdb == 'all':\n",
    "                execute_all_models(sdb, set_window, set_batch, set_epoch, cleaned_df, smn, sba, \"random\", date_chronological_vazio)\n",
    "            elif sdb == 'A1':\n",
    "                execute_all_models(sdb, set_window, set_batch, set_epoch, cleaned_df, smn, sba, \"chronological\", date_chronological_A1)\n",
    "            elif sdb == 'A2':\n",
    "                execute_all_models(sdb, set_window, set_batch, set_epoch, cleaned_df, smn, sba, \"chronological\", date_chronological_A2)\n",
    "            elif sdb == 'A3':\n",
    "                execute_all_models(sdb, set_window, set_batch, set_epoch, cleaned_df, smn, sba, \"chronological\", date_chronological_A3)\n",
    "            elif sdb == 'A4':\n",
    "                execute_all_models(sdb, set_window, set_batch, set_epoch, cleaned_df, smn, sba, \"chronological\",  date_chronological_A4)\n",
    "            \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
